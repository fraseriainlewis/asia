[
  {
    "objectID": "strategy/index.html",
    "href": "strategy/index.html",
    "title": "",
    "section": "",
    "text": "Code\nThis package is a collection of heavily documented Rmarkdown vignettes demonstrating how to use Tensorflow Probability (TFP) in RStudio for Bayesian inference. RStan features in many of these for comparison. The application focus is model building and trial simulation for clinical research and drug development."
  },
  {
    "objectID": "strategy/index.html#why-tensorflow-probability-tfp",
    "href": "strategy/index.html#why-tensorflow-probability-tfp",
    "title": "",
    "section": "Why Tensorflow Probability (TFP)?",
    "text": "Why Tensorflow Probability (TFP)?\n\nHaving alternative options, e.g., to RStan, when using open source reduces business risk\nTFP is developed and maintained by Google and is available globally\nTFP provides a wide variety of building blocks for probabilistic modelling and ML"
  },
  {
    "objectID": "strategy/index.html#running-the-vignettes",
    "href": "strategy/index.html#running-the-vignettes",
    "title": "",
    "section": "Running the Vignettes",
    "text": "Running the Vignettes\nLinks to the raw .Rmd files for download and local compilation can be found in each of the vignettes on this site. Local compilation has some dependencies which are given below.\n\nQuickstart\n\nInstall the tensorflow library and its install script must include “pandas” in the extra_packages option (see details below)\nInstall tfprobability\nEnsure pak is installed and loaded into session\nthen at R Console &gt; pak::pkg_install(\"fraseriainlewis/tfclinical\")\n\n\n\n\n\nInstallation details for Linux\nPython needs to be installed, and pyenv can also be a very useful tool if dealing with multiple Python versions and to avoid impacting the system Python installation.\n##########################################################\n# in RStudio Terminal (not R Console) or in Bash\nsudo apt-get update\nsudo apt-get install python3-venv python3-pip python3-dev\n##########################################################\n# in RStudio console\ninstall.packages(\"tensorflow\")\nlibrary(tensorflow)\ninstall_tensorflow(extra_packages = c(\"tf_keras\", \"tensorflow\", \"tensorflow-probability\",\"pandas\",\"matplotlib\"))\n# note - matplotlib is not essential, and may be removed or added later via py_install()\n#restart session\ninstall.packages(\"tfprobability\")\nlibrary(tensorflow)\nlibrary(tfprobability)\nd &lt;- tfd_binomial(total_count = 7, probs = 0.3) # if this works then tensorflow us correctly installed\n##########################################################\n\n\nInstallation details for Windows\nOn Windows the key part is to have a suitable Python installation and also that RStudio can locate this. Once this is in place then the installation process is as above for the Linux case.\n\n\nAdvanced: Installation of separate Python venv\nTo run the live Graphical Neural Network (GNN) vignette requires a separate Python venv to be setup because some tensorflow spin-off projects, such as TF-GNN, have strict compatibility requirements. Library versions likely need hardcoded as pip will not choose suitable combinations. The instructions below setup an appropriate venv for running the GNN vignette (only tested on MacOS).\n# in bash\npython3 -m venv gnn\nsource gnn/bin/activate\n# now install via pip the specific library versions needed\npip install tensorflow==2.16.2 tf_keras==2.16.0 tensorflow-gnn\nThe second part below in the R Console is to instruct reticulate to use this Python venv rather than one of the existing virtualenv environments, otherwise reticulate may use, e.g. the tensorflow venv created when doing the above install, which will likely not contain a compatible tensorflow version.\n# in RStudio console\nlibrary(reticulate)\nuse_virtualenv(\"/Users/work/gnn\", required = TRUE) # tell R to use the python interpreter and libraries in here"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code\nThis package is a collection of heavily documented Rmarkdown vignettes demonstrating how to use Tensorflow Probability (TFP) in RStudio for Bayesian inference. RStan features in many of these for comparison. The application focus is model building and trial simulation for clinical research and drug development."
  },
  {
    "objectID": "index.html#why-tensorflow-probability-tfp",
    "href": "index.html#why-tensorflow-probability-tfp",
    "title": "",
    "section": "Why Tensorflow Probability (TFP)?",
    "text": "Why Tensorflow Probability (TFP)?\n\nHaving alternative options, e.g., to RStan, when using open source reduces business risk\nTFP is developed and maintained by Google and is available globally\nTFP provides a wide variety of building blocks for probabilistic modelling and ML"
  },
  {
    "objectID": "index.html#running-the-vignettes",
    "href": "index.html#running-the-vignettes",
    "title": "",
    "section": "Running the Vignettes",
    "text": "Running the Vignettes\nLinks to the raw .Rmd files for download and local compilation can be found in each of the vignettes on this site. Local compilation has some dependencies which are given below.\n\nQuickstart\n\nInstall the tensorflow library and its install script must include “pandas” in the extra_packages option (see details below)\nInstall tfprobability\nEnsure pak is installed and loaded into session\nthen at R Console &gt; pak::pkg_install(\"fraseriainlewis/tfclinical\")\n\n\n\n\n\nInstallation details for Linux\nPython needs to be installed, and pyenv can also be a very useful tool if dealing with multiple Python versions and to avoid impacting the system Python installation.\n##########################################################\n# in RStudio Terminal (not R Console) or in Bash\nsudo apt-get update\nsudo apt-get install python3-venv python3-pip python3-dev\n##########################################################\n# in RStudio console\ninstall.packages(\"tensorflow\")\nlibrary(tensorflow)\ninstall_tensorflow(extra_packages = c(\"tf_keras\", \"tensorflow\", \"tensorflow-probability\",\"pandas\",\"matplotlib\"))\n# note - matplotlib is not essential, and may be removed or added later via py_install()\n#restart session\ninstall.packages(\"tfprobability\")\nlibrary(tensorflow)\nlibrary(tfprobability)\nd &lt;- tfd_binomial(total_count = 7, probs = 0.3) # if this works then tensorflow us correctly installed\n##########################################################\n\n\nInstallation details for Windows\nOn Windows the key part is to have a suitable Python installation and also that RStudio can locate this. Once this is in place then the installation process is as above for the Linux case.\n\n\nAdvanced: Installation of separate Python venv\nTo run the live Graphical Neural Network (GNN) vignette requires a separate Python venv to be setup because some tensorflow spin-off projects, such as TF-GNN, have strict compatibility requirements. Library versions likely need hardcoded as pip will not choose suitable combinations. The instructions below setup an appropriate venv for running the GNN vignette (only tested on MacOS).\n# in bash\npython3 -m venv gnn\nsource gnn/bin/activate\n# now install via pip the specific library versions needed\npip install tensorflow==2.16.2 tf_keras==2.16.0 tensorflow-gnn\nThe second part below in the R Console is to instruct reticulate to use this Python venv rather than one of the existing virtualenv environments, otherwise reticulate may use, e.g. the tensorflow venv created when doing the above install, which will likely not contain a compatible tensorflow version.\n# in RStudio console\nlibrary(reticulate)\nuse_virtualenv(\"/Users/work/gnn\", required = TRUE) # tell R to use the python interpreter and libraries in here"
  },
  {
    "objectID": "other/index_other.html",
    "href": "other/index_other.html",
    "title": "",
    "section": "",
    "text": "Code\nThis package is a collection of heavily documented Rmarkdown vignettes demonstrating how to use Tensorflow Probability (TFP) in RStudio for Bayesian inference. RStan features in many of these for comparison. The application focus is model building and trial simulation for clinical research and drug development."
  },
  {
    "objectID": "other/index_other.html#why-tensorflow-probability-tfp",
    "href": "other/index_other.html#why-tensorflow-probability-tfp",
    "title": "",
    "section": "Why Tensorflow Probability (TFP)?",
    "text": "Why Tensorflow Probability (TFP)?\n\nHaving alternative options, e.g., to RStan, when using open source reduces business risk\nTFP is developed and maintained by Google and is available globally\nTFP provides a wide variety of building blocks for probabilistic modelling and ML"
  },
  {
    "objectID": "other/index_other.html#running-the-vignettes",
    "href": "other/index_other.html#running-the-vignettes",
    "title": "",
    "section": "Running the Vignettes",
    "text": "Running the Vignettes\nLinks to the raw .Rmd files for download and local compilation can be found in each of the vignettes on this site. Local compilation has some dependencies which are given below.\n\nQuickstart\n\nInstall the tensorflow library and its install script must include “pandas” in the extra_packages option (see details below)\nInstall tfprobability\nEnsure pak is installed and loaded into session\nthen at R Console &gt; pak::pkg_install(\"fraseriainlewis/tfclinical\")\n\n\n\n\n\nInstallation details for Linux\nPython needs to be installed, and pyenv can also be a very useful tool if dealing with multiple Python versions and to avoid impacting the system Python installation.\n##########################################################\n# in RStudio Terminal (not R Console) or in Bash\nsudo apt-get update\nsudo apt-get install python3-venv python3-pip python3-dev\n##########################################################\n# in RStudio console\ninstall.packages(\"tensorflow\")\nlibrary(tensorflow)\ninstall_tensorflow(extra_packages = c(\"tf_keras\", \"tensorflow\", \"tensorflow-probability\",\"pandas\",\"matplotlib\"))\n# note - matplotlib is not essential, and may be removed or added later via py_install()\n#restart session\ninstall.packages(\"tfprobability\")\nlibrary(tensorflow)\nlibrary(tfprobability)\nd &lt;- tfd_binomial(total_count = 7, probs = 0.3) # if this works then tensorflow us correctly installed\n##########################################################\n\n\nInstallation details for Windows\nOn Windows the key part is to have a suitable Python installation and also that RStudio can locate this. Once this is in place then the installation process is as above for the Linux case.\n\n\nAdvanced: Installation of separate Python venv\nTo run the live Graphical Neural Network (GNN) vignette requires a separate Python venv to be setup because some tensorflow spin-off projects, such as TF-GNN, have strict compatibility requirements. Library versions likely need hardcoded as pip will not choose suitable combinations. The instructions below setup an appropriate venv for running the GNN vignette (only tested on MacOS).\n# in bash\npython3 -m venv gnn\nsource gnn/bin/activate\n# now install via pip the specific library versions needed\npip install tensorflow==2.16.2 tf_keras==2.16.0 tensorflow-gnn\nThe second part below in the R Console is to instruct reticulate to use this Python venv rather than one of the existing virtualenv environments, otherwise reticulate may use, e.g. the tensorflow venv created when doing the above install, which will likely not contain a compatible tensorflow version.\n# in RStudio console\nlibrary(reticulate)\nuse_virtualenv(\"/Users/work/gnn\", required = TRUE) # tell R to use the python interpreter and libraries in here"
  },
  {
    "objectID": "tfp/intro_model_build.html",
    "href": "tfp/intro_model_build.html",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "",
    "text": "how to pass data sets from R to TFP (Python)\nhow to build a simple hierarchical Bayesian model\n\nsetup the priors\ndefine the log-likelihood\ngenerate samples from this model\nwrite a function to compute the log_posterior (needed for MCMC sampling)\n\nhow setup a NUTS MCMC sampler\n\nwith parameter specific adaptive step-sizes\nmultiple chains\n\nhow to generate samples from the posterior\n\nincluding trace functions to compute in-stream diagnostics and custom outputs\n\n\nThe use case for this worked example is a two arm clinical trial with binary endpoint.\nClick here for the full R Markdown file that generated all the outputs shown here."
  },
  {
    "objectID": "tfp/intro_model_build.html#key-features-covered-in-vignette",
    "href": "tfp/intro_model_build.html#key-features-covered-in-vignette",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "",
    "text": "how to pass data sets from R to TFP (Python)\nhow to build a simple hierarchical Bayesian model\n\nsetup the priors\ndefine the log-likelihood\ngenerate samples from this model\nwrite a function to compute the log_posterior (needed for MCMC sampling)\n\nhow setup a NUTS MCMC sampler\n\nwith parameter specific adaptive step-sizes\nmultiple chains\n\nhow to generate samples from the posterior\n\nincluding trace functions to compute in-stream diagnostics and custom outputs\n\n\nThe use case for this worked example is a two arm clinical trial with binary endpoint.\nClick here for the full R Markdown file that generated all the outputs shown here."
  },
  {
    "objectID": "tfp/intro_model_build.html#overview",
    "href": "tfp/intro_model_build.html#overview",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "Overview",
    "text": "Overview\nThis vignette shows some of the key building blocks in building a Bayesian hierarchical model (BHM) in TFP. The current model could be used for a two arm clinical trial with a binary endpoint. This model will be expanded to a basket trial design in a later vignette as the necessary revisions are fairly minor.\nThe focus here is on some of the key building blocks in building a Bayesian model in TFP. A mixture of R and Python Rmarkdown cells are used, where R is generally used for data set creation and generation of figures, and Python for model building in TFP. This is a general theme of the vignettes; Python is used when direct interaction with the TFP API is needed, otherwise R is used as the assumed go-to language of choice for statisticians. Currently some plots use Python matplotlib but wll be replaced in due course with ggplot2.\n\nExample Model - Formulation\nThe model implemented here as an introduction to TFP is the following non-centered parameterization for a logistic model. A non-centered parameterization can be preferable when sampling from hierarchical Bayesian models as discussed in this article on the Stan website.\n\\[\n\\begin{aligned}\n\\mu_0 &\\sim \\text{Normal}(0, 2.5)\\\\\n\\sigma_0 &\\sim \\text{Half-Normal}(0, 2.5) \\\\\n\\mu_1 &\\sim \\text{Normal}(0, 2.5) \\\\\n\\sigma_1 &\\sim \\text{Half-Normal}(0, 2.5) \\\\\n\\tilde{\\theta}_j &\\sim \\text{Normal}(0, 1)\\quad\\enspace\\text{for}\\enspace{j=1,2}  \\\\\n\\beta_0 &= \\mu_0 + \\sigma_0 \\tilde{\\theta}_1 \\\\\n\\beta_1 &= \\mu_1 + \\sigma_1 \\tilde{\\theta}_2 \\\\\n\\text{logit(}{p_i}) &= \\beta_0 + \\beta_1 z_i\\quad\\quad\\enspace\\enspace\\text{for}\\enspace{i=1,\\dots,N}  \\\\\ny_i &\\sim \\text{Bernoulli}(p_i)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "tfp/intro_model_build.html#make-data-available-to-tfp-models",
    "href": "tfp/intro_model_build.html#make-data-available-to-tfp-models",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "Make data available to TFP models",
    "text": "Make data available to TFP models\nAssuming the data to be modelled, i.e. the source data from which parameters are to be estimated via model fitting, are loaded into R or generated in R then a first step is to make these data available to TFP. This functionality is readily provided by reticulate, with one watch-out - the Python pandas library is required to deal with data.frames or tibbles (see installation instructions).\nIn TFP models are written in Python but they use TF tensors as arguments, rather than more familiar Python structures such as numpy arrays. This distinction is important as tensors are strongly typed and so care is needed, including when moving back and forth to R via reticulate.\n\nExample data set\nThe R chunk below creates a simple dataset of three cols:\n\na binary response variable (0/1 = non-responder/responder),\na binary treatment variable (0/1 = control/test treatment)\nand a basket ID variable. (1)\n\nThe basket ID is currently set fixed at 1, denoting there is only one basket in this trial, i.e. a classical two arm randomized trial design. Baskets will be added in other vignettes.\n\n\nCode\n### Prepare model inputs\nset.seed(9999)\n# Set up data\nrr_k_ctrl &lt;- c(0.60)        # control response rate for each basket\nrr_k_trt &lt;- c(0.58)         # treatment response rate for each basket\n\nK&lt;-length(rr_k_ctrl)        # number of baskets\n\nN_k_ctrl &lt;- rep(250, K)     # number of control participants per basket\nN_k_trt &lt;- rep(250, K)      # number of treatment participants per basket\nN_k &lt;- N_k_ctrl + N_k_trt   # number of participants per basket (both arms combined)\nN &lt;- sum(N_k)               # total sample size\nk_vec &lt;- rep(1:K, N_k)      # N x 1 vector of basket indicators (1 to K)\n\nz_vec&lt;-NULL;\ny&lt;-NULL;\nfor(i in 1:K){ # for each basket repeat 0-control 1-trt according to the specifc Ns\n  z_vec&lt;-c(z_vec,rep(0:1,c(N_k_ctrl[i],N_k_trt[i]))) # treatment/control indicator\n  y&lt;-c(y,\n       c(rbinom(N_k_ctrl[i],1,rr_k_ctrl[i]), # bernoulli for control\n         rbinom(N_k_trt[i],1,rr_k_trt[i]))) #           for trt\n}\n\nthedata&lt;-data.frame(y,basketID=k_vec,Treatment=z_vec)\n\npy$thedata&lt;-r_to_py(thedata) # THE KEY LINE - makes data available to Python\n                             # this is converted into a pandas dataframe object in Python\n\n\n\n\nCode\nmy_table1_object &lt;- head(thedata) %&gt;%\n  kable() %&gt;%\n  kable_styling(full_width = F) %&gt;%\n  column_spec(1, bold = T, color = \"blue\")\n\nmy_table1_object\n\n\n\n\n\ny\nbasketID\nTreatment\n\n\n\n\n0\n1\n0\n\n\n0\n1\n0\n\n\n0\n1\n0\n\n\n1\n1\n0\n\n\n0\n1\n0\n\n\n0\n1\n0\n\n\n\n\n\nCode\n# Save the object\n#saveRDS(my_table1_object, \"precomputed/table1.rds\")"
  },
  {
    "objectID": "tfp/intro_model_build.html#setup-python-and-get-data-from-r",
    "href": "tfp/intro_model_build.html#setup-python-and-get-data-from-r",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "Setup python and get data from R",
    "text": "Setup python and get data from R\nWe first load in the necessary libraries for TFP and then convert the dataset passed from R into tensors. Note that as tensors are strongly typed we explicitly define the storage type (dtype).\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport os\nimport keras\n\n\n/Users/work/tfp/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n  if not hasattr(np, \"object\"):\n\n\nCode\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow_probability import distributions as tfd\ntfb = tfp.bijectors\nimport warnings\nimport time\nimport sys\n\n## The data.frame passed is now a panda df but for TPF this needs to be further\n## converted into tensors here we simply convert each col of the data.frame into\n## a Rank 1 tensor (i.e. a vector)\ny_data=tf.convert_to_tensor(thedata.iloc[:,0], dtype = tf.float32)\nk_vec=tf.convert_to_tensor(thedata.iloc[:,1], dtype = tf.float32)\nz_vec=tf.convert_to_tensor(thedata.iloc[:,2], dtype = tf.float32)\n\n#print(f\"y={z_vec}\") #if using locally this prints out to R console"
  },
  {
    "objectID": "tfp/intro_model_build.html#defining-the-model",
    "href": "tfp/intro_model_build.html#defining-the-model",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "Defining the Model",
    "text": "Defining the Model\nWe now fully define our model, i.e., the data likelihood and all the prior densities. We use a TFP function called JointDistributionSequentialAutoBatched. There are a number of alternative TFP functions for defining Bayesian models. This is one of the simplest but still capable of defining complex models. The model definition in JointDistributionSequentialAutoBatched must follow a strict convention which is explained in the comments.\n\nDefine the likelihood\n\n\nCode\n# Define LIKELIHOOD - this is defined first, as functions need defined before used\n# The arguments in our function are in the REVERSE ORDER that the parameters\n# appear in JointDistrbutionSequential this is essential\ndef make_observed_dist(log_odd_control_and_ratio,sigma1, mu1,sigma0,mu0):\n    # beta is a two element tensor, beta[0] parameter for the log odds of the control arm effect\n    #                               beta[1] log odds ratio of treatment to control\n    beta=tf.stack([mu0 + sigma0 * log_odd_control_and_ratio[0],\n                   mu1 + sigma1 * log_odd_control_and_ratio[1]])\n                   #tf.stack is used to stack tensors without creating new tensors\n\n    # return a vector of Bernoulli probabilities, one for each patient, and these estimates\n    # dependent on which arm the patient was randomized to, i.e. two unique values\n    return(tfd.Independent(\n        tfd.Bernoulli(logits=beta[0]+beta[1]*z_vec), #define as logits\n        reinterpreted_batch_ndims = 1 # This tells TFP that log_prob here is a single value\n                                      # equal to sum of individual log_probs, a vector\n                                      # this is usual when defining a data likelihood\n    ))\n\n\n\n\nDefine the full model\nThis is straightforward provided it’s done in sequence. Care is needed in understanding whether parameters are scalars or vectors, especially in more complex models which feature a design matrix and matrix algebra. Simulating a realization from the model is a simple way to see what the structure of the state variable of the model is. In more complex models simulating multiple values is useful to check the structure is as expected.\n\n\nCode\n# DEFINE FULL MODEL - hyperparams hard coded\n# model is y[i] = Bernoulli(p[i]) where logit(p[i]) = intercept + treatment*z[i]\nmodel = tfd.JointDistributionSequentialAutoBatched([\n  tfd.Normal(loc=0., scale=2.5, name=\"mu0\"),  # prior for mean of control arm log odds\n  tfd.HalfNormal(scale=2.5, name=\"sigma0\"),   # prior for sd of control arm log odds\n  tfd.Normal(loc=0., scale=2.5, name=\"mu1\"),  # prior for mean of treatment log odds ratio\n  tfd.HalfNormal(scale=2.5, name=\"sigma1\"),   # prior for sd of treatment log odds ratio\n  # now come to the standard normals resulting from non-centred parameterization\n  # define these as vector MVN with identity scale matrix\n  tfd.Normal(loc=tf.zeros(2), scale=tf.ones(2), name=\"log_odd_control_and_ratio\"),\n  # now finally define the likelihood as we have already defined parameters in this\n  make_observed_dist # the data likelihood defined as a function above\n])\n\ntf.random.set_seed(9999)\n# Sample one variate from this joint probability model - shows what structure the model\n# produces/needs. i.e. what the state variable of the model looks like in TF\nmysample=model.sample(1) # 1 = one random variate\nprint(f\"{mysample}\")\n\n\n[&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.0810314], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.47621235], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5767661], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.9815397], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.0181222 , -0.39404973]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1, 500), dtype=int32, numpy=\narray([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]], dtype=int32)&gt;]\n\n\n\n\nCode\nprint(py$mysample)\n\n\n[[1]]\ntf.Tensor([-1.0810314], shape=(1), dtype=float32)\n\n[[2]]\ntf.Tensor([0.47621235], shape=(1), dtype=float32)\n\n[[3]]\ntf.Tensor([0.5767661], shape=(1), dtype=float32)\n\n[[4]]\ntf.Tensor([3.9815397], shape=(1), dtype=float32)\n\n[[5]]\ntf.Tensor([[ 0.0181222  -0.39404973]], shape=(1, 2), dtype=float32)\n\n[[6]]\ntf.Tensor(\n[[0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1\n  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0\n  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0\n  1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0\n  0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0\n  1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0\n  0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0\n  0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n  0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n  0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0]], shape=(1, 500), dtype=int32)\n\n\n\n\nDefine the log_prog_fn\nTo use the various MCMC samplers provided in TFP we need to explicitly provide a function which returns the log of the posterior density. This is straightforward and simply involves wrapping around the existing model method log_prob() as seen below.\n\n\nCode\n# the ordering of the arguments in this must match exactly the ordering used in\n# JointDistributionSequentialAutoBatched\ndef log_prob_fn(mu0, sigma0, mu1,sigma1, log_odd_control_and_ratio):\n  return model.log_prob((\n      mu0, sigma0, mu1,sigma1, log_odd_control_and_ratio,y_data))\n                                                         ## last argument is the DATA (y)\n\n# useful to see how to call this. We use the simulated values above\nmylogp=log_prob_fn(mysample[0], mysample[1], mysample[2],mysample[3], mysample[4])\n\n\n\n\nCode\nprint(py$mylogp)\n\n\ntf.Tensor([-573.09534], shape=(1), dtype=float32)"
  },
  {
    "objectID": "tfp/intro_model_build.html#setup-the-mcmc-sampler",
    "href": "tfp/intro_model_build.html#setup-the-mcmc-sampler",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "Setup the MCMC sampler",
    "text": "Setup the MCMC sampler\nTFP generally has a lower level interface to MCMC sampler routines compared to Stan, and as such requires a few more manual steps. There are also a range of different ways to setup the samplers. The API is evolving so that higher level functions are steadily being introduced.\n\nNUTS and Adaptive-Step Size\nWe use a No-u-turn sampler, and add into this dual step size adaptation for efficiency. To set this up we need to do the following:\n\nDefine the bijectors needed for each parameter in the model (as NUTS requires unconstrained mappings)\nDefine tensors in a specific way such that we allow:\n\neach parameter to have its own step size adaptation\nadditionally allow this tuning to be done independently for each chain\n\n\nThe following code sets this up. The precise tensor structure needed can require some trial and error. It has to be carefully specified as the structure tells TFP how to arrange the calculations, both in terms of computational efficiency (parallelization wherever possible) and also at what level (parameter, chain, parameter*chain) to perform step size adaptation.\nThe initial step sizes chosen here (which are then adapted) are somewhat arbitrary and making good choices here is something that could be borrowed from how RStan does this.\n\n\nCode\n# bijectors which define the mapping from **unconstrained space to target space**\n# i.e. Exp() is used for scale as this maps R -&gt; R+\nunconstraining_bijectors = [\n    tfb.Identity(),  # mu0\n    tfb.Exp(),       # sigma0\n    tfb.Identity(),  # mu1\n    tfb.Exp(),       # sigma0\n    tfb.Identity()  # log_odd_control_and_ratio - this is a vector parameter\n]\n\n## Adaptive step size.\n## Do in two parts, first a structure to allow step size to adapt separately for\n## each parameter\nsteps=[tf.constant([0.5]),                # mu0\n        tf.constant([0.05]),              # sigma0\n        tf.constant([0.5]),               # mu1\n        tf.constant([0.05]),              # sigma1\n        tf.constant([[0.5,0.5]]) # log_odd_control_and_ratio\n                                 # NOTE - vector and must have correct shape\n        ]\n\n## now we replicate the above \"steps\" array into a structure where this is\n## repeated inside each chain\nn_chains=2 ## THIS SETS NUMBER OF CHAINS\nsteps_chains = [tf.expand_dims(tf.repeat(steps[0],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(steps[1],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(steps[2],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(steps[3],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.tile(steps[4],[n_chains,1]),axis=1) # starts with shape (1,2) i.e. [[a, b]]\n                 ]\n\n\n\n\nCode\nprint(py$steps_chains)\n\n\n[[1]]\ntf.Tensor(\n[[0.5]\n [0.5]], shape=(2, 1), dtype=float32)\n\n[[2]]\ntf.Tensor(\n[[0.05]\n [0.05]], shape=(2, 1), dtype=float32)\n\n[[3]]\ntf.Tensor(\n[[0.5]\n [0.5]], shape=(2, 1), dtype=float32)\n\n[[4]]\ntf.Tensor(\n[[0.05]\n [0.05]], shape=(2, 1), dtype=float32)\n\n[[5]]\ntf.Tensor(\n[[[0.5 0.5]]\n\n [[0.5 0.5]]], shape=(2, 1, 2), dtype=float32)\n\n\n\n\nNo U-Turn Sampler\nNow define the No U-Turn sampler, which needs several steps: - define the parameters in the No U-Turn sampler, - then wrap this inside the TransformedTransitionKernel which tell the NUTS what transformations (bijectors - as defined above) to use - then wrap the NUTS and TransformedTransitionKernel inside the adaptive step size routine\n\n\nCode\nnum_results=1000 # number of steps to run each chain for AFTER burn-in\nnum_burnin_steps=100 # this is discarded (currently not other option in TPF)\n\nmysampler=tfp.mcmc.NoUTurnSampler(\n                                     target_log_prob_fn=log_prob_fn,\n                                     max_tree_depth=15, # default is 10\n                                     max_energy_diff=1000.0, # default do not change\n                                     step_size=steps_chains\n                                     )\n\nsampler = tfp.mcmc.TransformedTransitionKernel( # inside this the starting conditions must be on\n                                                # original scale i.e. precisions must be &gt;0\n    mysampler,\n    bijector=unconstraining_bijectors\n    )\n\n## define final sampler - NUTS, bijectors and adaptation\nadaptive_sampler = tfp.mcmc.DualAveragingStepSizeAdaptation(\n    inner_kernel=sampler,\n    num_adaptation_steps=int(0.8 * num_burnin_steps),\n    reduce_fn=tfp.math.reduce_logmeanexp, # default - this determines how to change the step\n                                          # adaptation across chains\n    #reduce_fn=tfp.math.reduce_log_harmonic_mean_exp, # might be better if difficult chains\n    target_accept_prob=tf.cast(0.95, tf.float32)) # this is a key parameter to get good mixing"
  },
  {
    "objectID": "tfp/intro_model_build.html#define-starting-point-for-chains",
    "href": "tfp/intro_model_build.html#define-starting-point-for-chains",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "Define starting point for chains",
    "text": "Define starting point for chains\nExplicit initial conditions are needed for each parameter in each chain. This is currently done very simply via hard coding. See RStan manual for a fairly simple way to generate random conditions that often works well. The key point here is that the initial conditions must be again in a specific tensor structure.\n\n\nCode\nistate=[tf.constant([0.0]),\n        tf.constant([0.5]),\n                   tf.constant([0.0]),\n        tf.constant([0.5]),\n        tf.constant([[1.,-1.]])]\n\ncurrent_state = [tf.expand_dims(tf.repeat(istate[0],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(istate[1],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(istate[2],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(istate[3],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.tile(istate[4],[n_chains,1]),axis=1) # starts with shape (1,2) i.e. [[a, b]]\n                 ]\nprint(current_state)\n\n\n[&lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[0.],\n       [0.]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[0.5],\n       [0.5]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[0.],\n       [0.]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[0.5],\n       [0.5]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(2, 1, 2), dtype=float32, numpy=\narray([[[ 1., -1.]],\n\n       [[ 1., -1.]]], dtype=float32)&gt;]\n\n\n\n\nCode\nprint(py$current_state)\n\n\n[[1]]\ntf.Tensor(\n[[0.]\n [0.]], shape=(2, 1), dtype=float32)\n\n[[2]]\ntf.Tensor(\n[[0.5]\n [0.5]], shape=(2, 1), dtype=float32)\n\n[[3]]\ntf.Tensor(\n[[0.]\n [0.]], shape=(2, 1), dtype=float32)\n\n[[4]]\ntf.Tensor(\n[[0.5]\n [0.5]], shape=(2, 1), dtype=float32)\n\n[[5]]\ntf.Tensor(\n[[[ 1. -1.]]\n\n [[ 1. -1.]]], shape=(2, 1, 2), dtype=float32)"
  },
  {
    "objectID": "tfp/intro_model_build.html#perform-mcmc-sampling",
    "href": "tfp/intro_model_build.html#perform-mcmc-sampling",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "Perform MCMC Sampling",
    "text": "Perform MCMC Sampling\nWe are now in a position to actually sample from our model, but first we will set up a tracer function which is called at every step. This can be used either for diagnostics, such as monitor step-size changes, or else to generate custom output, such as compute log-likelihood at each step. Such a function can be computationally expensive and so it’s a trade-off as to whether to wait until the samples have been generated to compute additional functions of the parameters of interest.\n\n\nCode\ndef trace_fn(state, pkr):\n    return {\n        'sample': state, # this is also pkr['all'][0].transformed_state[0]\n        'step_size': pkr.new_step_size,  # &lt;--- THIS extracts the adapted step size\n        'all': pkr, #state is also pkr['all'][0].transformed_state[0]\n        #pkr is a named tuple with ._fields = ('transformed_state', 'inner_results')\n        # 'transformed_state is the state\n        # 'inner_results' is diagnostics\n        # ('target_log_prob', 'grads_target_log_prob', 'step_size', 'log_accept_ratio', 'leapfrogs_taken',                     'is_accepted', 'reach_max_depth', 'has_divergence', 'energy', 'seed')\n        'has_divergence':pkr[0].inner_results.has_divergence,\n        'logL': log_prob_fn(state[0], state[1],state[2],state[3],state[4])\n    }\n\n\nNow run the actual sampler. The number of steps and burn-in were defined above when we setup the No U-Turn sampler.\n\n\nCode\n# Speed up sampling by tracing with `tf.function`.\n@tf.function(autograph=False, jit_compile=True,reduce_retracing=True)\ndef do_sampling():\n  return tfp.mcmc.sample_chain(\n      kernel=adaptive_sampler,\n      current_state=current_state,\n      num_results=num_results,\n      num_burnin_steps=num_burnin_steps,\n      seed= tf.constant([9999, 9999], dtype=tf.int32), # this is random seed\n      trace_fn=trace_fn)\n\n\nt0 = time.time()\n#samples, kernel_results = do_sampling()\nsamples, traceout = do_sampling()\n#res = do_sampling()\n#[mu0, sigma0, mu1, sigma1,log_odds_control_and_ratio], results = do_sampling()\nt1 = time.time()\nprint(\"Inference ran in {:.2f}s.\".format(t1-t0))\n\n\nInference ran in 15.49s.\n\n\nCode\n## there is a trailing dimension of 1 so need to squeeze to get each col is chain, and each row is parameter sample\nsamplesflat = list(map(lambda x: tf.squeeze(x).numpy(), samples))\nprint(f\" means for mu0, sigma0, mu1, sigma1\\n\")\n\n\n means for mu0, sigma0, mu1, sigma1\n\n\nCode\nthemeans=[np.mean(row) for row in samplesflat[0:4]]\n\n\n\n\nCode\n# this shows how the step size changes during adaptation\n#traceout['step_size']\n#traceout['step_size'][0][999] #step size for mu0 at iteration 999+1\n\n\n\n\nCode\nnames(py$themeans)&lt;-c(\"mu0\",\"sigma0\",\"mu1\",\"sigma1\")\nprint(py$themeans)\n\n\n$mu0\n[1] 0.35283\n\n$sigma0\n[1] 1.561941\n\n$mu1\n[1] -0.2059595\n\n$sigma1\n[1] 1.568997\n\n\n\nSome Output plots\nWe plot the log-likelihood over the MCMC steps (post-burn-in) using two chains, a different colour for each chain. Similarly for trace plots. These are just illustrative output examples, the number of burn-in and total chain steps here are too low for reliable estimation.\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 5))\nplt.plot(tf.squeeze(traceout['logL'].numpy()))\nplt.title(\"log likelihood\")\n#plt.savefig(f\"precomputed/vg1_loglike.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, axes = plt.subplots(2, 2)#, sharex='col', sharey='col')\nfig.set_size_inches(10, 5)\naxes[0][0].plot(samplesflat[0]) # mu_b\naxes[0][1].plot(samplesflat[1]) # tau_b\naxes[1][0].plot(samplesflat[2]) # mu\naxes[1][1].plot(samplesflat[3]) # tau\n#plt.savefig(f\"precomputed/vg1_traces.png\")\nplt.show()"
  },
  {
    "objectID": "vignettes/index.html",
    "href": "vignettes/index.html",
    "title": "",
    "section": "",
    "text": "Code\nThis package is a collection of heavily documented Rmarkdown vignettes demonstrating how to use Tensorflow Probability (TFP) in RStudio for Bayesian inference. RStan features in many of these for comparison. The application focus is model building and trial simulation for clinical research and drug development.",
    "crumbs": [
      "Other",
      "Why Tensorflow Probability (TFP)?"
    ]
  },
  {
    "objectID": "vignettes/index.html#why-tensorflow-probability-tfp",
    "href": "vignettes/index.html#why-tensorflow-probability-tfp",
    "title": "",
    "section": "Why Tensorflow Probability (TFP)?",
    "text": "Why Tensorflow Probability (TFP)?\n\nHaving alternative options, e.g., to RStan, when using open source reduces business risk\nTFP is developed and maintained by Google and is available globally\nTFP provides a wide variety of building blocks for probabilistic modelling and ML",
    "crumbs": [
      "Other",
      "Why Tensorflow Probability (TFP)?"
    ]
  },
  {
    "objectID": "vignettes/index.html#running-the-vignettes",
    "href": "vignettes/index.html#running-the-vignettes",
    "title": "",
    "section": "Running the Vignettes",
    "text": "Running the Vignettes\nLinks to the raw .Rmd files for download and local compilation can be found in each of the vignettes on this site. Local compilation has some dependencies which are given below.\n\nQuickstart\n\nInstall the tensorflow library and its install script must include “pandas” in the extra_packages option (see details below)\nInstall tfprobability\nEnsure pak is installed and loaded into session\nthen at R Console &gt; pak::pkg_install(\"fraseriainlewis/tfclinical\")\n\n\n\n\n\nInstallation details for Linux\nPython needs to be installed, and pyenv can also be a very useful tool if dealing with multiple Python versions and to avoid impacting the system Python installation.\n##########################################################\n# in RStudio Terminal (not R Console) or in Bash\nsudo apt-get update\nsudo apt-get install python3-venv python3-pip python3-dev\n##########################################################\n# in RStudio console\ninstall.packages(\"tensorflow\")\nlibrary(tensorflow)\ninstall_tensorflow(extra_packages = c(\"tf_keras\", \"tensorflow\", \"tensorflow-probability\",\"pandas\",\"matplotlib\"))\n# note - matplotlib is not essential, and may be removed or added later via py_install()\n#restart session\ninstall.packages(\"tfprobability\")\nlibrary(tensorflow)\nlibrary(tfprobability)\nd &lt;- tfd_binomial(total_count = 7, probs = 0.3) # if this works then tensorflow us correctly installed\n##########################################################\n\n\nInstallation details for Windows\nOn Windows the key part is to have a suitable Python installation and also that RStudio can locate this. Once this is in place then the installation process is as above for the Linux case.\n\n\nAdvanced: Installation of separate Python venv\nTo run the live Graphical Neural Network (GNN) vignette requires a separate Python venv to be setup because some tensorflow spin-off projects, such as TF-GNN, have strict compatibility requirements. Library versions likely need hardcoded as pip will not choose suitable combinations. The instructions below setup an appropriate venv for running the GNN vignette (only tested on MacOS).\n# in bash\npython3 -m venv gnn\nsource gnn/bin/activate\n# now install via pip the specific library versions needed\npip install tensorflow==2.16.2 tf_keras==2.16.0 tensorflow-gnn\nThe second part below in the R Console is to instruct reticulate to use this Python venv rather than one of the existing virtualenv environments, otherwise reticulate may use, e.g. the tensorflow venv created when doing the above install, which will likely not contain a compatible tensorflow version.\n# in RStudio console\nlibrary(reticulate)\nuse_virtualenv(\"/Users/work/gnn\", required = TRUE) # tell R to use the python interpreter and libraries in here",
    "crumbs": [
      "Other",
      "Why Tensorflow Probability (TFP)?"
    ]
  },
  {
    "objectID": "tfp/index_tfp.html",
    "href": "tfp/index_tfp.html",
    "title": "",
    "section": "",
    "text": "Code\nThis package is a collection of heavily documented Rmarkdown vignettes demonstrating how to use Tensorflow Probability (TFP) in RStudio for Bayesian inference. RStan features in many of these for comparison. The application focus is model building and trial simulation for clinical research and drug development."
  },
  {
    "objectID": "tfp/index_tfp.html#why-tensorflow-probability-tfp",
    "href": "tfp/index_tfp.html#why-tensorflow-probability-tfp",
    "title": "",
    "section": "Why Tensorflow Probability (TFP)?",
    "text": "Why Tensorflow Probability (TFP)?\n\nHaving alternative options, e.g., to RStan, when using open source reduces business risk\nTFP is developed and maintained by Google and is available globally\nTFP provides a wide variety of building blocks for probabilistic modelling and ML"
  },
  {
    "objectID": "tfp/index_tfp.html#running-the-vignettes",
    "href": "tfp/index_tfp.html#running-the-vignettes",
    "title": "",
    "section": "Running the Vignettes",
    "text": "Running the Vignettes\nLinks to the raw .Rmd files for download and local compilation can be found in each of the vignettes on this site. Local compilation has some dependencies which are given below.\n\nQuickstart\n\nInstall the tensorflow library and its install script must include “pandas” in the extra_packages option (see details below)\nInstall tfprobability\nEnsure pak is installed and loaded into session\nthen at R Console &gt; pak::pkg_install(\"fraseriainlewis/tfclinical\")\n\n\n\n\n\nInstallation details for Linux\nPython needs to be installed, and pyenv can also be a very useful tool if dealing with multiple Python versions and to avoid impacting the system Python installation.\n##########################################################\n# in RStudio Terminal (not R Console) or in Bash\nsudo apt-get update\nsudo apt-get install python3-venv python3-pip python3-dev\n##########################################################\n# in RStudio console\ninstall.packages(\"tensorflow\")\nlibrary(tensorflow)\ninstall_tensorflow(extra_packages = c(\"tf_keras\", \"tensorflow\", \"tensorflow-probability\",\"pandas\",\"matplotlib\"))\n# note - matplotlib is not essential, and may be removed or added later via py_install()\n#restart session\ninstall.packages(\"tfprobability\")\nlibrary(tensorflow)\nlibrary(tfprobability)\nd &lt;- tfd_binomial(total_count = 7, probs = 0.3) # if this works then tensorflow us correctly installed\n##########################################################\n\n\nInstallation details for Windows\nOn Windows the key part is to have a suitable Python installation and also that RStudio can locate this. Once this is in place then the installation process is as above for the Linux case.\n\n\nAdvanced: Installation of separate Python venv\nTo run the live Graphical Neural Network (GNN) vignette requires a separate Python venv to be setup because some tensorflow spin-off projects, such as TF-GNN, have strict compatibility requirements. Library versions likely need hardcoded as pip will not choose suitable combinations. The instructions below setup an appropriate venv for running the GNN vignette (only tested on MacOS).\n# in bash\npython3 -m venv gnn\nsource gnn/bin/activate\n# now install via pip the specific library versions needed\npip install tensorflow==2.16.2 tf_keras==2.16.0 tensorflow-gnn\nThe second part below in the R Console is to instruct reticulate to use this Python venv rather than one of the existing virtualenv environments, otherwise reticulate may use, e.g. the tensorflow venv created when doing the above install, which will likely not contain a compatible tensorflow version.\n# in RStudio console\nlibrary(reticulate)\nuse_virtualenv(\"/Users/work/gnn\", required = TRUE) # tell R to use the python interpreter and libraries in here"
  },
  {
    "objectID": "rstan/index_rstan.html",
    "href": "rstan/index_rstan.html",
    "title": "",
    "section": "",
    "text": "Code\nThis package is a collection of heavily documented Rmarkdown vignettes demonstrating how to use Tensorflow Probability (TFP) in RStudio for Bayesian inference. RStan features in many of these for comparison. The application focus is model building and trial simulation for clinical research and drug development."
  },
  {
    "objectID": "rstan/index_rstan.html#why-tensorflow-probability-tfp",
    "href": "rstan/index_rstan.html#why-tensorflow-probability-tfp",
    "title": "",
    "section": "Why Tensorflow Probability (TFP)?",
    "text": "Why Tensorflow Probability (TFP)?\n\nHaving alternative options, e.g., to RStan, when using open source reduces business risk\nTFP is developed and maintained by Google and is available globally\nTFP provides a wide variety of building blocks for probabilistic modelling and ML"
  },
  {
    "objectID": "rstan/index_rstan.html#running-the-vignettes",
    "href": "rstan/index_rstan.html#running-the-vignettes",
    "title": "",
    "section": "Running the Vignettes",
    "text": "Running the Vignettes\nLinks to the raw .Rmd files for download and local compilation can be found in each of the vignettes on this site. Local compilation has some dependencies which are given below.\n\nQuickstart\n\nInstall the tensorflow library and its install script must include “pandas” in the extra_packages option (see details below)\nInstall tfprobability\nEnsure pak is installed and loaded into session\nthen at R Console &gt; pak::pkg_install(\"fraseriainlewis/tfclinical\")\n\n\n\n\n\nInstallation details for Linux\nPython needs to be installed, and pyenv can also be a very useful tool if dealing with multiple Python versions and to avoid impacting the system Python installation.\n##########################################################\n# in RStudio Terminal (not R Console) or in Bash\nsudo apt-get update\nsudo apt-get install python3-venv python3-pip python3-dev\n##########################################################\n# in RStudio console\ninstall.packages(\"tensorflow\")\nlibrary(tensorflow)\ninstall_tensorflow(extra_packages = c(\"tf_keras\", \"tensorflow\", \"tensorflow-probability\",\"pandas\",\"matplotlib\"))\n# note - matplotlib is not essential, and may be removed or added later via py_install()\n#restart session\ninstall.packages(\"tfprobability\")\nlibrary(tensorflow)\nlibrary(tfprobability)\nd &lt;- tfd_binomial(total_count = 7, probs = 0.3) # if this works then tensorflow us correctly installed\n##########################################################\n\n\nInstallation details for Windows\nOn Windows the key part is to have a suitable Python installation and also that RStudio can locate this. Once this is in place then the installation process is as above for the Linux case.\n\n\nAdvanced: Installation of separate Python venv\nTo run the live Graphical Neural Network (GNN) vignette requires a separate Python venv to be setup because some tensorflow spin-off projects, such as TF-GNN, have strict compatibility requirements. Library versions likely need hardcoded as pip will not choose suitable combinations. The instructions below setup an appropriate venv for running the GNN vignette (only tested on MacOS).\n# in bash\npython3 -m venv gnn\nsource gnn/bin/activate\n# now install via pip the specific library versions needed\npip install tensorflow==2.16.2 tf_keras==2.16.0 tensorflow-gnn\nThe second part below in the R Console is to instruct reticulate to use this Python venv rather than one of the existing virtualenv environments, otherwise reticulate may use, e.g. the tensorflow venv created when doing the above install, which will likely not contain a compatible tensorflow version.\n# in RStudio console\nlibrary(reticulate)\nuse_virtualenv(\"/Users/work/gnn\", required = TRUE) # tell R to use the python interpreter and libraries in here"
  },
  {
    "objectID": "models/index.html",
    "href": "models/index.html",
    "title": "",
    "section": "",
    "text": "Code\nThis package is a collection of heavily documented Rmarkdown vignettes demonstrating how to use Tensorflow Probability (TFP) in RStudio for Bayesian inference. RStan features in many of these for comparison. The application focus is model building and trial simulation for clinical research and drug development."
  },
  {
    "objectID": "models/index.html#why-tensorflow-probability-tfp",
    "href": "models/index.html#why-tensorflow-probability-tfp",
    "title": "",
    "section": "Why Tensorflow Probability (TFP)?",
    "text": "Why Tensorflow Probability (TFP)?\n\nHaving alternative options, e.g., to RStan, when using open source reduces business risk\nTFP is developed and maintained by Google and is available globally\nTFP provides a wide variety of building blocks for probabilistic modelling and ML"
  },
  {
    "objectID": "models/index.html#running-the-vignettes",
    "href": "models/index.html#running-the-vignettes",
    "title": "",
    "section": "Running the Vignettes",
    "text": "Running the Vignettes\nLinks to the raw .Rmd files for download and local compilation can be found in each of the vignettes on this site. Local compilation has some dependencies which are given below.\n\nQuickstart\n\nInstall the tensorflow library and its install script must include “pandas” in the extra_packages option (see details below)\nInstall tfprobability\nEnsure pak is installed and loaded into session\nthen at R Console &gt; pak::pkg_install(\"fraseriainlewis/tfclinical\")\n\n\n\n\n\nInstallation details for Linux\nPython needs to be installed, and pyenv can also be a very useful tool if dealing with multiple Python versions and to avoid impacting the system Python installation.\n##########################################################\n# in RStudio Terminal (not R Console) or in Bash\nsudo apt-get update\nsudo apt-get install python3-venv python3-pip python3-dev\n##########################################################\n# in RStudio console\ninstall.packages(\"tensorflow\")\nlibrary(tensorflow)\ninstall_tensorflow(extra_packages = c(\"tf_keras\", \"tensorflow\", \"tensorflow-probability\",\"pandas\",\"matplotlib\"))\n# note - matplotlib is not essential, and may be removed or added later via py_install()\n#restart session\ninstall.packages(\"tfprobability\")\nlibrary(tensorflow)\nlibrary(tfprobability)\nd &lt;- tfd_binomial(total_count = 7, probs = 0.3) # if this works then tensorflow us correctly installed\n##########################################################\n\n\nInstallation details for Windows\nOn Windows the key part is to have a suitable Python installation and also that RStudio can locate this. Once this is in place then the installation process is as above for the Linux case.\n\n\nAdvanced: Installation of separate Python venv\nTo run the live Graphical Neural Network (GNN) vignette requires a separate Python venv to be setup because some tensorflow spin-off projects, such as TF-GNN, have strict compatibility requirements. Library versions likely need hardcoded as pip will not choose suitable combinations. The instructions below setup an appropriate venv for running the GNN vignette (only tested on MacOS).\n# in bash\npython3 -m venv gnn\nsource gnn/bin/activate\n# now install via pip the specific library versions needed\npip install tensorflow==2.16.2 tf_keras==2.16.0 tensorflow-gnn\nThe second part below in the R Console is to instruct reticulate to use this Python venv rather than one of the existing virtualenv environments, otherwise reticulate may use, e.g. the tensorflow venv created when doing the above install, which will likely not contain a compatible tensorflow version.\n# in RStudio console\nlibrary(reticulate)\nuse_virtualenv(\"/Users/work/gnn\", required = TRUE) # tell R to use the python interpreter and libraries in here"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\nCode\n1 + 1\n\n\n[1] 2"
  }
]