[
  {
    "objectID": "tfp/intro_normal_tf_stan.html",
    "href": "tfp/intro_normal_tf_stan.html",
    "title": "Bayesian Linear Regression TF v rstanarm",
    "section": "",
    "text": "how to build a simple Bayesian linear regression model\n\nsetup the design matrix\ngenerate MLE estimates as initial conditions for MCMC\n\ncomparison of numerical results against rstanarm\nshow how to match rstanarm’s centering of predictors and auto-scaled priors\n\nThis example uses simulated data.\nClick here for the full R Markdown file that generated all the outputs shown here."
  },
  {
    "objectID": "tfp/intro_normal_tf_stan.html#key-features-covered-in-vignette",
    "href": "tfp/intro_normal_tf_stan.html#key-features-covered-in-vignette",
    "title": "Bayesian Linear Regression TF v rstanarm",
    "section": "",
    "text": "how to build a simple Bayesian linear regression model\n\nsetup the design matrix\ngenerate MLE estimates as initial conditions for MCMC\n\ncomparison of numerical results against rstanarm\nshow how to match rstanarm’s centering of predictors and auto-scaled priors\n\nThis example uses simulated data.\nClick here for the full R Markdown file that generated all the outputs shown here."
  },
  {
    "objectID": "tfp/intro_normal_tf_stan.html#example-data-set",
    "href": "tfp/intro_normal_tf_stan.html#example-data-set",
    "title": "Bayesian Linear Regression TF v rstanarm",
    "section": "Example data set",
    "text": "Example data set\nThis example uses a simple simulated data set generated as follows:\n\n\nCode\nset.seed(1001)\nnperarm&lt;-25 # N patients per arm\nT1 &lt;-rep(c(0,1),nperarm); # treatment arm 1/0 (binary)\nX1&lt;-rnorm(n=2*nperarm,mean=10,sd=2) # continuous covariate\ny &lt;- rnorm(n=2*nperarm,mean=1+X1+2*T1,sd=2) # Normal response\n\nthedata&lt;-data.frame(y=y,X=X1,T=T1)\n\n\n\n\nCode\nmy_table1_object &lt;- head(thedata) %&gt;%\n  kable() %&gt;%\n  kable_styling(full_width = F) %&gt;%\n  column_spec(1, bold = T, color = \"darkgreen\")\n\n# Save the object\n#saveRDS(my_table1_object, \"precomputed/vg3_table1.rds\")\n\n\n\nExample Model - Formulation\nStandard additive model with Gaussian response and identity link function with priors for the intercept, slope and scale parameters.\n\\[\n\\begin{aligned}\n\\text{y}_i &\\sim \\text{N}(\\mu_i, \\phi^2) \\\\\n\\mu_i &= \\alpha + \\beta_{\\text{1}} X_{i} + \\beta_{\\text{2}} T_{i}\\quad\\text{for } i=1,\\dots,N \\\\\n\\alpha &\\sim \\text{N}(0, a_{h}) \\\\\n\\beta_{\\text{1}}, \\beta_{\\text{2}} &\\sim \\text{N}(0, b_{h}) \\\\\n\\sigma &\\sim \\text{Exponential}(c_{h})\n\\end{aligned}\n\\] where \\(a_h, b_h\\) and \\(c_h\\) are fixed hyper-parameters."
  },
  {
    "objectID": "tfp/intro_normal_tf_stan.html#rstanarm",
    "href": "tfp/intro_normal_tf_stan.html#rstanarm",
    "title": "Bayesian Linear Regression TF v rstanarm",
    "section": "rstanarm",
    "text": "rstanarm\nWe first fit the model using rstanarm library using it’s high level interface. When using the rstanarm library it is important to read the specific documentation as:\n\ninternal predictor centering may be applied by default (including Normal response as here)\npredictor centering is also applied to binary variables and factors\nsome priors maybe auto-scaled by default\n\nCentering does not alter the parameter estimates but does impact how the model is coded, while the second does alter estimates but is designed to do so in a small (and sensible) way. But using TFP in R then it is easy to also utilize the auto-scaled priors computed by RStan (as below), allowing direct numerical comparisons between these two libraries.\nWe run the rstanarm Normal regression model and capture the priors used as some of these have been auto-scaled by stan_glm. These same priors will then be used in RStan and TFP. The only parameter auto-scaled is the scale parameter but the code shows how to do this for the slope parameters also (but not needed here).\n\n\nCode\n# Note on Rescaling\n# note for models that do internal predictor centering then need location shift back for intercept\n# i.e y = a + b*(x1-mean) + c*(x2-mean)\n# the interpretation of a on non-centered predictors is value of y at x1=0 and x2=0\n# so want a at x1=0 and x2=0 so E(y) = mean(a) + mean(b)*-mean(x1) + mean(c)*-mean(x2) etc.\n\n# Estimate Bayesian version with stan_glm\nstan_glm1 &lt;- stan_glm(y ~ X + T, data = thedata, family = gaussian(),\n                      prior = normal(0, 2.5),\n                      prior_intercept = normal(0, 5),\n                      seed = 12345,\n                      warmup = 10000,      # Number of warmup iterations per chain\n                      iter = 20000,        # Total iterations per chain (warmup + sampling)\n                      thin = 1,\n                      chains = 4,\n                      refresh=0) # turn off iteration output for compactness           \nres_m&lt;-as.matrix(stan_glm1)\n#summary(res_m[,\"(Intercept)\"])\n\n# now get the priors used by rstanarm for use in RStan and TFP\nprior_scales&lt;-prior_summary(stan_glm1)\n# get the predictor adjusted scale - stating priors explicitly so no autoscaling\nbeta_prior_scale&lt;-prior_scales$prior$scale\n#  aux is always re-scaled -next line always needed\nsd_prior_scale&lt;-prior_scales$prior_aux$adjusted_scale #need 1/sd_prior_scale for exp\n\n## now send the observed data and the new priors to python for use later with TFP\npy$thedata&lt;-r_to_py(thedata) \nrescaled_sd=c(beta_prior_scale,1/sd_prior_scale)\npy$rescaled_sd&lt;-r_to_py(rescaled_sd)"
  },
  {
    "objectID": "tfp/intro_normal_tf_stan.html#rstan",
    "href": "tfp/intro_normal_tf_stan.html#rstan",
    "title": "Bayesian Linear Regression TF v rstanarm",
    "section": "RStan",
    "text": "RStan\nWe now reproduce the same analysis in RStan rather than rstanarm as this requires more model specific details which can then be implemented in TFP. - we pass the computed priors from rstanarm above which we extracted from the stanfit results object\n\n\nCode\n# RStan code\nset.seed(12345)\n\n# --- Define the Stan model as a string in R ---\n\nstan_model_string &lt;- \"\ndata {\n  int&lt;lower=1&gt; N;                 // Number of observations\n  int&lt;lower=1&gt; M;                 //number of predictors excl intercept\n  array[N] real&lt;lower=0&gt; y;         // Response variable (counts)\n  vector[N] X;           // Continuous predictor\n  vector[N] T;           // Binary predictor\n  //Hyperparameters\n  array[M] real&lt;lower=0&gt;rescaled_sd;// to hold auto-scaled priors from rstanarm\n\n}\n\ntransformed data {\n  vector[N] X_centered;\n  real mean_X=mean(X);\n  vector[N] T_centered;\n  real mean_T=mean(T);\n  X_centered = X - mean_X;  // Center in transformed data block\n  T_centered = T - mean_T;\n}\n\nparameters {\n  real alpha;                     // Intercept\n  real beta_X;               // Coefficient for roach1\n  real beta_T;            // Coefficient for treatment\n  real&lt;lower=0&gt; phi;              // scale parameter\n}\n\ntransformed parameters {\n  array[N] real mu;           \n  for (i in 1:N) {\n    // this is mean for each data point and will go into likelihood function\n    mu[i] = alpha +\n                beta_X * X_centered[i] +\n                beta_T * T_centered[i] ;\n  }\n}\n\nmodel {\n  // --- Priors ---\n  // Weakly informative priors for coefficients and intercept\n  alpha ~ normal(0, 5.0);           // Prior for intercept\n  // the next parameters are passed from R as taken from rstanarm\n  beta_X ~ normal(0,rescaled_sd[1] );     // Prior for wt coefficient\n  beta_T ~ normal(0, rescaled_sd[2]);  // Prior for am coefficient\n  phi ~ exponential(rescaled_sd[3]); // Prior for scale\n\n  // --- Likelihood ---\n  y ~  normal(mu, phi);\n}\n\ngenerated quantities {\n  // compute intercept paramater back on non-centred axes\n  real intercept_0;\n  intercept_0=alpha + beta_X*-mean_X + beta_T*-mean_T;\n}\n\"\n\n# --- Prepare data for Stan ---\n# The data needs to be provided as a list for rstan::stan()\nstan_data &lt;- list(\n  N = nrow(thedata),\n  M = 3, # number of passed hyperpriors\n  rescaled_sd=c(beta_prior_scale,1/sd_prior_scale),# 1/ as prior uses rate\n  y = thedata$y,\n  X = thedata$X,\n  T = thedata$T\n)\n\n# --- Fit the Stan model ---\n# Use rstan::stan() to compile and sample from the model\nfit &lt;- stan(\n  model_code = stan_model_string,\n  data = stan_data,\n  chains = 4,         # Number of MCMC chains\n  warmup = 10000,      # Number of warmup iterations per chain\n  iter = 20000,        # Total iterations per chain (warmup + sampling)\n  thin = 1,           # Thinning rate\n  seed = 12345,          # For reproducibility\n  refresh=0,\n  control = list(adapt_delta = 0.95, max_treedepth = 15) # Adjust for sampling issues if needed\n)\n#&gt; Trying to compile a simple C file\n#&gt; Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\n#&gt; using C compiler: ‘Apple clang version 17.0.0 (clang-1700.6.3.2)’\n#&gt; using SDK: ‘MacOSX26.2.sdk’\n#&gt; clang -arch arm64 -std=gnu2x -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\n#&gt; In file included from &lt;built-in&gt;:1:\n#&gt; In file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\n#&gt; In file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\n#&gt; In file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n#&gt; /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#&gt;   679 | #include &lt;cmath&gt;\n#&gt;       |          ^~~~~~~\n#&gt; 1 error generated.\n#&gt; make: *** [foo.o] Error 1\n\n# --- 5. Extract main parameters and produce density plots ---\nres2&lt;-extract(fit,par=c(\"alpha\",\"beta_X\",\" beta_T\",\"phi\",\"intercept_0\"))"
  },
  {
    "objectID": "tfp/intro_normal_tf_stan.html#comparison-of-results-rstanarm-v-rstan",
    "href": "tfp/intro_normal_tf_stan.html#comparison-of-results-rstanarm-v-rstan",
    "title": "Bayesian Linear Regression TF v rstanarm",
    "section": "Comparison of results rstanarm v RStan",
    "text": "Comparison of results rstanarm v RStan\nThe plots below show a simple comparison of densities between rstanarm and RStan, and it’s clear these are virtually identical as we hoped. We now implement the same model into TFP"
  },
  {
    "objectID": "tfp/intro_normal_tf_stan.html#tfp",
    "href": "tfp/intro_normal_tf_stan.html#tfp",
    "title": "Bayesian Linear Regression TF v rstanarm",
    "section": "TFP",
    "text": "TFP\nWe now implement the above model in TFP\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport os\nimport keras\n#&gt; /Users/work/tfp/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n#&gt;   if not hasattr(np, \"object\"):\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow_probability import distributions as tfd\ntfb = tfp.bijectors\nimport warnings\nimport time\nimport sys\n\n## The data.frame passed is now a panda df but for TPF this needs to be further\n## converted into tensors here we simply convert each col of the data.frame into\n## a Rank 1 tensor (i.e. a vector)\ny_data=tf.convert_to_tensor(thedata.iloc[:,0], dtype = tf.float32)\nX_data=tf.convert_to_tensor(thedata.iloc[:,1]-np.mean(thedata.iloc[:,1]), dtype = tf.float32)\nT_data=tf.convert_to_tensor(thedata.iloc[:,2]-np.mean(thedata.iloc[:,2]), dtype = tf.float32)\n\nrescaled_sd=tf.convert_to_tensor(rescaled_sd, dtype = tf.float32)\n# rescaled_sd is also available here\n\n\n\n\nCode\n# Define LIKELIHOOD - this is defined first, as functions need defined before used\n# The arguments in our function are in the REVERSE ORDER that the parameters\n# appear in JointDistrbutionSequential this is essential\ndef make_observed_dist(phi,beta_T,beta_X,alpha):\n      # Compute linear mean: mu = alpha + beta * x\n      # When sampling 3 times:\n      # alpha: (3,), beta: (3,), x_data: (10,)\n      # Need to broadcast to (3, 10)\n      alpha_expanded = tf.expand_dims(alpha, -1)  # (3, 1)\n      beta_X_expanded = tf.expand_dims(beta_X, -1)    # (3, 1)\n      beta_T_expanded = tf.expand_dims(beta_T, -1)    # (3, 1)\n      mu = alpha_expanded + beta_X_expanded * X_data + beta_T_expanded * T_data  # (3, 1) + (3, 1) * (10,) = (3, 10)\n      # Expand sigma to broadcast\n      phi_expanded = tf.expand_dims(phi, -1)  # (3, 1)\n      # return a vector of Gaussian probabilities, one for each obs\n      return(tfd.Independent(\n        tfd.Normal(loc = mu, scale = phi_expanded), #define as logits\n        reinterpreted_batch_ndims = 1 # This tells TFP that log_prob here is a single value\n                                      # equal to sum of individual log_probs, a vector\n                                      # this is usual when defining a data likelihood\n      ))\n\n\n\n\nCode\n# DEFINE FULL MODEL - hyperparams hard coded\n# model is y[i] = Bernoulli(p[i]) where logit(p[i]) = intercept + treatment*z[i]\nmodel = tfd.JointDistributionSequentialAutoBatched([\n  tfd.Normal(loc=0., scale=5.0, name=\"alpha\"),  # prior intercept after centering\n  tfd.Normal(loc=0.,scale=rescaled_sd[[0]], name=\"beta_X\"),   # prior slops\n  tfd.Normal(loc=0., scale=rescaled_sd[[1]], name=\"beta_T\"),  # prior for slope\n  tfd.Exponential(rate=rescaled_sd[[2]], name=\"phi\"), # prior for scale\n  # now finally define the likelihood as we have already defined parameters in this\n  make_observed_dist # the data likelihood defined as a function above\n])\n\ntf.random.set_seed(9999)\n# Sample one variate from this joint probability model - shows what structure the model\n# produces/needs. i.e. what the state variable of the model looks like in TF\nmysample=model.sample(1) # 1 = one random variate\n#print(f\"{mysample}\")\n\n\n\n\nCode\nprint(py$mysample)\n#&gt; [[1]]\n#&gt; tf.Tensor([-2.162063], shape=(1), dtype=float32)\n#&gt; \n#&gt; [[2]]\n#&gt; tf.Tensor([-0.47621235], shape=(1), dtype=float32)\n#&gt; \n#&gt; [[3]]\n#&gt; tf.Tensor([0.5767661], shape=(1), dtype=float32)\n#&gt; \n#&gt; [[4]]\n#&gt; tf.Tensor([4.2605343], shape=(1), dtype=float32)\n#&gt; \n#&gt; [[5]]\n#&gt; tf.Tensor(\n#&gt; [[ -4.3874145   -3.3130984   -0.15982056   7.79828     -3.4587793\n#&gt;    -3.1774316   -5.998131    -6.7247553   -4.397325     5.481294\n#&gt;    -9.06718     -5.8871617   -0.2163018   -1.9762998    0.95448446\n#&gt;    -0.760056    -9.579603     3.8263593   -0.6446984  -10.433388\n#&gt;     5.0122633   -4.2138934   -6.930196    -0.45444846  -0.60048294\n#&gt;    -3.3823504    5.5510936   -5.9933496    3.005193     1.27367\n#&gt;    -8.595339    -0.9684988  -10.7711735   -4.492715     4.0445337\n#&gt;     0.06356597  -2.8085861    0.19860196  -1.2367177   -4.812044\n#&gt;     5.1663084    0.9606662   -2.761689    -6.958252    -5.048527\n#&gt;     4.4922323    3.2969918   -4.7693863   -7.007819    -8.508112  ]], shape=(1, 50), dtype=float32)\n\n\n\nDefine the log_prog_fn\nTo use the various MCMC samplers provided in TFP we need to explicitly provide a function which returns the log of the posterior density. This is straightforward and simply involves wrapping around the existing model method log_prob() as seen below.\n\n\nCode\n# the ordering of the arguments in this must match exactly the ordering used in\n# JointDistributionSequentialAutoBatched\ndef log_prob_fn(alpha,beta_X,beta_T,phi):\n  return model.log_prob((\n      alpha,beta_X,beta_T,phi,y_data))\n                                                         ## last argument is the DATA (y)\n\n# useful to see how to call this. We use the simulated values above\nmylogp=log_prob_fn(mysample[0], mysample[1], mysample[2],mysample[3])\n\n\n\n\nCode\nprint(py$mylogp)\n#&gt; tf.Tensor([-428.61087], shape=(1), dtype=float32)\n\n\n\n\nCode\ndef neg_log_prob_fn(pars):\n    alpha=pars[[0]]\n    beta_X=pars[[1]]\n    beta_T=pars[[2]]\n    phi=pars[[3]]\n    \"\"\"Unnormalized target density as a function of states.\"\"\"\n    return -model.log_prob((\n      alpha, beta_X,beta_T,phi, y_data))\n\n\n#### get starting values by find MLE\nif(True):\n    start = tf.constant([0.1,0.1,0.1,0.1],dtype = tf.float32)  # Starting point for the search.\n    optim_results = tfp.optimizer.nelder_mead_minimize(neg_log_prob_fn,\n                 initial_vertex=start, func_tolerance=1e-04,max_iterations=1000)\n\n    print(optim_results.initial_objective_values)\n    print(optim_results.objective_value)\n    print(optim_results.position)\n#&gt; tf.Tensor([380042.6  379745.22 379903.53 380026.53 344706.5 ], shape=(5,), dtype=float32)\n#&gt; tf.Tensor(118.10513, shape=(), dtype=float32)\n#&gt; tf.Tensor([11.955216   0.9763453  2.2308257  2.03263  ], shape=(4,), dtype=float32)\n\n\n\n\nCode\n# bijector to map contrained parameters to real\nunconstraining_bijectors = [\n    tfb.Identity(),\n    tfb.Identity(),\n    tfb.Identity(),\n    tfb.Exp()\n]\n\nnum_results=20000\nnum_burnin_steps=10000\n\nsampler = tfp.mcmc.TransformedTransitionKernel(\n    tfp.mcmc.NoUTurnSampler(\n        target_log_prob_fn=log_prob_fn,\n        step_size=tf.cast(0.5, tf.float32)), #tf.cast(0.1, tf.float32)),\n    bijector=unconstraining_bijectors\n    )\n\nadaptive_sampler = tfp.mcmc.DualAveragingStepSizeAdaptation(\n    inner_kernel=sampler,\n    num_adaptation_steps=int(0.8 * num_burnin_steps),\n    target_accept_prob=tf.cast(0.8, tf.float32))\n\nistate = optim_results.position\n\nn_chains=4\ncurrent_state = [tf.expand_dims(tf.repeat(istate[0],repeats=n_chains,axis=-1),axis=-1),\n                 tf.expand_dims(tf.repeat(istate[1],repeats=n_chains,axis=-1),axis=-1),\n                 tf.expand_dims(tf.repeat(istate[2],repeats=n_chains,axis=-1),axis=-1),\n                 tf.expand_dims(tf.repeat(istate[3],repeats=n_chains,axis=-1),axis=-1)\n                 ]\n\n# Speed up sampling by tracing with `tf.function`.\n@tf.function(autograph=False, jit_compile=True,reduce_retracing=True)\ndef do_sampling():\n  return tfp.mcmc.sample_chain(\n      kernel=adaptive_sampler,\n      current_state=current_state,\n      num_results=num_results,\n      num_burnin_steps=num_burnin_steps,\n      seed= tf.constant([9199, 9999], dtype=tf.int32),\n      trace_fn=None)#lambda current_state, kernel_results: kernel_results)\n\n\nt0 = time.time()\n#samples, kernel_results = do_sampling()\nsamples = do_sampling()\nt1 = time.time()\nprint(\"Inference ran in {:.2f}s.\".format(t1-t0))\n#&gt; Inference ran in 3.71s.\n\nsamples = list(map(lambda x: tf.squeeze(x).numpy(), samples))\n\n\n\n\nCode\nalpha = tf.reshape(samples[0], [-1])\nbeta_X = tf.reshape(samples[1], [-1])\nbeta_T = tf.reshape(samples[2], [-1])\nphi = tf.reshape(samples[3], [-1])\n\n# need to re-locate prior back to original - reverse centering\nintercept0=alpha + beta_X*-np.mean(thedata.iloc[:,1]) + beta_T*-np.mean(thedata.iloc[:,2])"
  },
  {
    "objectID": "tfp/intro_model_build.html",
    "href": "tfp/intro_model_build.html",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "",
    "text": "how to pass data sets from R to TFP (Python)\nhow to build a simple hierarchical Bayesian model\n\nsetup the priors\ndefine the log-likelihood\ngenerate samples from this model\nwrite a function to compute the log_posterior (needed for MCMC sampling)\n\nhow setup a NUTS MCMC sampler\n\nwith parameter specific adaptive step-sizes\nmultiple chains\n\nhow to generate samples from the posterior\n\nincluding trace functions to compute in-stream diagnostics and custom outputs\n\n\nThe use case for this worked example is a two arm clinical trial with binary endpoint.\nClick here for the full R Markdown file that generated all the outputs shown here."
  },
  {
    "objectID": "tfp/intro_model_build.html#key-features-covered-in-vignette",
    "href": "tfp/intro_model_build.html#key-features-covered-in-vignette",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "",
    "text": "how to pass data sets from R to TFP (Python)\nhow to build a simple hierarchical Bayesian model\n\nsetup the priors\ndefine the log-likelihood\ngenerate samples from this model\nwrite a function to compute the log_posterior (needed for MCMC sampling)\n\nhow setup a NUTS MCMC sampler\n\nwith parameter specific adaptive step-sizes\nmultiple chains\n\nhow to generate samples from the posterior\n\nincluding trace functions to compute in-stream diagnostics and custom outputs\n\n\nThe use case for this worked example is a two arm clinical trial with binary endpoint.\nClick here for the full R Markdown file that generated all the outputs shown here."
  },
  {
    "objectID": "tfp/intro_model_build.html#overview",
    "href": "tfp/intro_model_build.html#overview",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "Overview",
    "text": "Overview\nThis vignette shows some of the key building blocks in building a Bayesian hierarchical model (BHM) in TFP. The current model could be used for a two arm clinical trial with a binary endpoint. This model will be expanded to a basket trial design in a later vignette as the necessary revisions are fairly minor.\nThe focus here is on some of the key building blocks in building a Bayesian model in TFP. A mixture of R and Python Rmarkdown cells are used, where R is generally used for data set creation and generation of figures, and Python for model building in TFP. This is a general theme of the vignettes; Python is used when direct interaction with the TFP API is needed, otherwise R is used as the assumed go-to language of choice for statisticians. Currently some plots use Python matplotlib but wll be replaced in due course with ggplot2.\n\nExample Model - Formulation\nThe model implemented here as an introduction to TFP is the following non-centered parameterization for a logistic model. A non-centered parameterization can be preferable when sampling from hierarchical Bayesian models as discussed in this article on the Stan website.\n\\[\n\\begin{aligned}\n\\mu_0 &\\sim \\text{Normal}(0, 2.5)\\\\\n\\sigma_0 &\\sim \\text{Half-Normal}(0, 2.5) \\\\\n\\mu_1 &\\sim \\text{Normal}(0, 2.5) \\\\\n\\sigma_1 &\\sim \\text{Half-Normal}(0, 2.5) \\\\\n\\tilde{\\theta}_j &\\sim \\text{Normal}(0, 1)\\quad\\enspace\\text{for}\\enspace{j=1,2}  \\\\\n\\beta_0 &= \\mu_0 + \\sigma_0 \\tilde{\\theta}_1 \\\\\n\\beta_1 &= \\mu_1 + \\sigma_1 \\tilde{\\theta}_2 \\\\\n\\text{logit(}{p_i}) &= \\beta_0 + \\beta_1 z_i\\quad\\quad\\enspace\\enspace\\text{for}\\enspace{i=1,\\dots,N}  \\\\\ny_i &\\sim \\text{Bernoulli}(p_i)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "tfp/intro_model_build.html#make-data-available-to-tfp-models",
    "href": "tfp/intro_model_build.html#make-data-available-to-tfp-models",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "Make data available to TFP models",
    "text": "Make data available to TFP models\nAssuming the data to be modelled, i.e. the source data from which parameters are to be estimated via model fitting, are loaded into R or generated in R then a first step is to make these data available to TFP. This functionality is readily provided by reticulate, with one watch-out - the Python pandas library is required to deal with data.frames or tibbles (see installation instructions).\nIn TFP models are written in Python but they use TF tensors as arguments, rather than more familiar Python structures such as numpy arrays. This distinction is important as tensors are strongly typed and so care is needed, including when moving back and forth to R via reticulate.\n\nExample data set\nThe R chunk below creates a simple dataset of three cols:\n\na binary response variable (0/1 = non-responder/responder),\na binary treatment variable (0/1 = control/test treatment)\nand a basket ID variable. (1)\n\nThe basket ID is currently set fixed at 1, denoting there is only one basket in this trial, i.e. a classical two arm randomized trial design. Baskets will be added in other vignettes.\n\n\nCode\n### Prepare model inputs\nset.seed(9999)\n# Set up data\nrr_k_ctrl &lt;- c(0.60)        # control response rate for each basket\nrr_k_trt &lt;- c(0.58)         # treatment response rate for each basket\n\nK&lt;-length(rr_k_ctrl)        # number of baskets\n\nN_k_ctrl &lt;- rep(250, K)     # number of control participants per basket\nN_k_trt &lt;- rep(250, K)      # number of treatment participants per basket\nN_k &lt;- N_k_ctrl + N_k_trt   # number of participants per basket (both arms combined)\nN &lt;- sum(N_k)               # total sample size\nk_vec &lt;- rep(1:K, N_k)      # N x 1 vector of basket indicators (1 to K)\n\nz_vec&lt;-NULL;\ny&lt;-NULL;\nfor(i in 1:K){ # for each basket repeat 0-control 1-trt according to the specifc Ns\n  z_vec&lt;-c(z_vec,rep(0:1,c(N_k_ctrl[i],N_k_trt[i]))) # treatment/control indicator\n  y&lt;-c(y,\n       c(rbinom(N_k_ctrl[i],1,rr_k_ctrl[i]), # bernoulli for control\n         rbinom(N_k_trt[i],1,rr_k_trt[i]))) #           for trt\n}\n\nthedata&lt;-data.frame(y,basketID=k_vec,Treatment=z_vec)\n\npy$thedata&lt;-r_to_py(thedata) # THE KEY LINE - makes data available to Python\n                             # this is converted into a pandas dataframe object in Python\n\n\n\n\nCode\nmy_table1_object &lt;- head(thedata) %&gt;%\n  kable() %&gt;%\n  kable_styling(full_width = F) %&gt;%\n  column_spec(1, bold = T, color = \"blue\")\n\nmy_table1_object\n\n\n\n\n\ny\nbasketID\nTreatment\n\n\n\n\n0\n1\n0\n\n\n0\n1\n0\n\n\n0\n1\n0\n\n\n1\n1\n0\n\n\n0\n1\n0\n\n\n0\n1\n0\n\n\n\n\n\nCode\n# Save the object\n#saveRDS(my_table1_object, \"precomputed/table1.rds\")"
  },
  {
    "objectID": "tfp/intro_model_build.html#setup-python-and-get-data-from-r",
    "href": "tfp/intro_model_build.html#setup-python-and-get-data-from-r",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "Setup python and get data from R",
    "text": "Setup python and get data from R\nWe first load in the necessary libraries for TFP and then convert the dataset passed from R into tensors. Note that as tensors are strongly typed we explicitly define the storage type (dtype).\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport os\nimport keras\n\n\nWARNING:tensorflow:From C:\\Users\\fil44768\\ONEDRI~1\\DOCUME~1\\VIRTUA~1\\R-TENS~1\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\n\nCode\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow_probability import distributions as tfd\ntfb = tfp.bijectors\nimport warnings\nimport time\nimport sys\n\n## The data.frame passed is now a panda df but for TPF this needs to be further\n## converted into tensors here we simply convert each col of the data.frame into\n## a Rank 1 tensor (i.e. a vector)\ny_data=tf.convert_to_tensor(thedata.iloc[:,0], dtype = tf.float32)\nk_vec=tf.convert_to_tensor(thedata.iloc[:,1], dtype = tf.float32)\nz_vec=tf.convert_to_tensor(thedata.iloc[:,2], dtype = tf.float32)\n\n#print(f\"y={z_vec}\") #if using locally this prints out to R console"
  },
  {
    "objectID": "tfp/intro_model_build.html#defining-the-model",
    "href": "tfp/intro_model_build.html#defining-the-model",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "Defining the Model",
    "text": "Defining the Model\nWe now fully define our model, i.e., the data likelihood and all the prior densities. We use a TFP function called JointDistributionSequentialAutoBatched. There are a number of alternative TFP functions for defining Bayesian models. This is one of the simplest but still capable of defining complex models. The model definition in JointDistributionSequentialAutoBatched must follow a strict convention which is explained in the comments.\n\nDefine the likelihood\n\n\nCode\n# Define LIKELIHOOD - this is defined first, as functions need defined before used\n# The arguments in our function are in the REVERSE ORDER that the parameters\n# appear in JointDistrbutionSequential this is essential\ndef make_observed_dist(log_odd_control_and_ratio,sigma1, mu1,sigma0,mu0):\n    # beta is a two element tensor, beta[0] parameter for the log odds of the control arm effect\n    #                               beta[1] log odds ratio of treatment to control\n    beta=tf.stack([mu0 + sigma0 * log_odd_control_and_ratio[0],\n                   mu1 + sigma1 * log_odd_control_and_ratio[1]])\n                   #tf.stack is used to stack tensors without creating new tensors\n\n    # return a vector of Bernoulli probabilities, one for each patient, and these estimates\n    # dependent on which arm the patient was randomized to, i.e. two unique values\n    return(tfd.Independent(\n        tfd.Bernoulli(logits=beta[0]+beta[1]*z_vec), #define as logits\n        reinterpreted_batch_ndims = 1 # This tells TFP that log_prob here is a single value\n                                      # equal to sum of individual log_probs, a vector\n                                      # this is usual when defining a data likelihood\n    ))\n\n\n\n\nDefine the full model\nThis is straightforward provided it’s done in sequence. Care is needed in understanding whether parameters are scalars or vectors, especially in more complex models which feature a design matrix and matrix algebra. Simulating a realization from the model is a simple way to see what the structure of the state variable of the model is. In more complex models simulating multiple values is useful to check the structure is as expected.\n\n\nCode\n# DEFINE FULL MODEL - hyperparams hard coded\n# model is y[i] = Bernoulli(p[i]) where logit(p[i]) = intercept + treatment*z[i]\nmodel = tfd.JointDistributionSequentialAutoBatched([\n  tfd.Normal(loc=0., scale=2.5, name=\"mu0\"),  # prior for mean of control arm log odds\n  tfd.HalfNormal(scale=2.5, name=\"sigma0\"),   # prior for sd of control arm log odds\n  tfd.Normal(loc=0., scale=2.5, name=\"mu1\"),  # prior for mean of treatment log odds ratio\n  tfd.HalfNormal(scale=2.5, name=\"sigma1\"),   # prior for sd of treatment log odds ratio\n  # now come to the standard normals resulting from non-centred parameterization\n  # define these as vector MVN with identity scale matrix\n  tfd.Normal(loc=tf.zeros(2), scale=tf.ones(2), name=\"log_odd_control_and_ratio\"),\n  # now finally define the likelihood as we have already defined parameters in this\n  make_observed_dist # the data likelihood defined as a function above\n])\n\ntf.random.set_seed(9999)\n# Sample one variate from this joint probability model - shows what structure the model\n# produces/needs. i.e. what the state variable of the model looks like in TF\nmysample=model.sample(1) # 1 = one random variate\nprint(f\"{mysample}\")\n\n\n[&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([-1.0810314], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.47621235], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5767661], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([3.9815397], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.0181222 , -0.39404973]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(1, 500), dtype=int32, numpy=\narray([[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]], dtype=int32)&gt;]\n\n\n\n\nCode\nprint(py$mysample)\n\n\n[[1]]\ntf.Tensor([-1.0810314], shape=(1), dtype=float32)\n\n[[2]]\ntf.Tensor([0.47621235], shape=(1), dtype=float32)\n\n[[3]]\ntf.Tensor([0.5767661], shape=(1), dtype=float32)\n\n[[4]]\ntf.Tensor([3.9815397], shape=(1), dtype=float32)\n\n[[5]]\ntf.Tensor([[ 0.0181222  -0.39404973]], shape=(1, 2), dtype=float32)\n\n[[6]]\ntf.Tensor(\n[[0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1\n  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0\n  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 0 0\n  1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0\n  0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0\n  1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0\n  0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0\n  0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0\n  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n  0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n  0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0]], shape=(1, 500), dtype=int32)\n\n\n\n\nDefine the log_prog_fn\nTo use the various MCMC samplers provided in TFP we need to explicitly provide a function which returns the log of the posterior density. This is straightforward and simply involves wrapping around the existing model method log_prob() as seen below.\n\n\nCode\n# the ordering of the arguments in this must match exactly the ordering used in\n# JointDistributionSequentialAutoBatched\ndef log_prob_fn(mu0, sigma0, mu1,sigma1, log_odd_control_and_ratio):\n  return model.log_prob((\n      mu0, sigma0, mu1,sigma1, log_odd_control_and_ratio,y_data))\n                                                         ## last argument is the DATA (y)\n\n# useful to see how to call this. We use the simulated values above\nmylogp=log_prob_fn(mysample[0], mysample[1], mysample[2],mysample[3], mysample[4])\n\n\n\n\nCode\nprint(py$mylogp)\n\n\ntf.Tensor([-573.09534], shape=(1), dtype=float32)"
  },
  {
    "objectID": "tfp/intro_model_build.html#setup-the-mcmc-sampler",
    "href": "tfp/intro_model_build.html#setup-the-mcmc-sampler",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "Setup the MCMC sampler",
    "text": "Setup the MCMC sampler\nTFP generally has a lower level interface to MCMC sampler routines compared to Stan, and as such requires a few more manual steps. There are also a range of different ways to setup the samplers. The API is evolving so that higher level functions are steadily being introduced.\n\nNUTS and Adaptive-Step Size\nWe use a No-u-turn sampler, and add into this dual step size adaptation for efficiency. To set this up we need to do the following:\n\nDefine the bijectors needed for each parameter in the model (as NUTS requires unconstrained mappings)\nDefine tensors in a specific way such that we allow:\n\neach parameter to have its own step size adaptation\nadditionally allow this tuning to be done independently for each chain\n\n\nThe following code sets this up. The precise tensor structure needed can require some trial and error. It has to be carefully specified as the structure tells TFP how to arrange the calculations, both in terms of computational efficiency (parallelization wherever possible) and also at what level (parameter, chain, parameter*chain) to perform step size adaptation.\nThe initial step sizes chosen here (which are then adapted) are somewhat arbitrary and making good choices here is something that could be borrowed from how RStan does this.\n\n\nCode\n# bijectors which define the mapping from **unconstrained space to target space**\n# i.e. Exp() is used for scale as this maps R -&gt; R+\nunconstraining_bijectors = [\n    tfb.Identity(),  # mu0\n    tfb.Exp(),       # sigma0\n    tfb.Identity(),  # mu1\n    tfb.Exp(),       # sigma0\n    tfb.Identity()  # log_odd_control_and_ratio - this is a vector parameter\n]\n\n## Adaptive step size.\n## Do in two parts, first a structure to allow step size to adapt separately for\n## each parameter\nsteps=[tf.constant([0.5]),                # mu0\n        tf.constant([0.05]),              # sigma0\n        tf.constant([0.5]),               # mu1\n        tf.constant([0.05]),              # sigma1\n        tf.constant([[0.5,0.5]]) # log_odd_control_and_ratio\n                                 # NOTE - vector and must have correct shape\n        ]\n\n## now we replicate the above \"steps\" array into a structure where this is\n## repeated inside each chain\nn_chains=2 ## THIS SETS NUMBER OF CHAINS\nsteps_chains = [tf.expand_dims(tf.repeat(steps[0],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(steps[1],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(steps[2],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(steps[3],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.tile(steps[4],[n_chains,1]),axis=1) # starts with shape (1,2) i.e. [[a, b]]\n                 ]\n\n\n\n\nCode\nprint(py$steps_chains)\n\n\n[[1]]\ntf.Tensor(\n[[0.5]\n [0.5]], shape=(2, 1), dtype=float32)\n\n[[2]]\ntf.Tensor(\n[[0.05]\n [0.05]], shape=(2, 1), dtype=float32)\n\n[[3]]\ntf.Tensor(\n[[0.5]\n [0.5]], shape=(2, 1), dtype=float32)\n\n[[4]]\ntf.Tensor(\n[[0.05]\n [0.05]], shape=(2, 1), dtype=float32)\n\n[[5]]\ntf.Tensor(\n[[[0.5 0.5]]\n\n [[0.5 0.5]]], shape=(2, 1, 2), dtype=float32)\n\n\n\n\nNo U-Turn Sampler\nNow define the No U-Turn sampler, which needs several steps: - define the parameters in the No U-Turn sampler, - then wrap this inside the TransformedTransitionKernel which tell the NUTS what transformations (bijectors - as defined above) to use - then wrap the NUTS and TransformedTransitionKernel inside the adaptive step size routine\n\n\nCode\nnum_results=1000 # number of steps to run each chain for AFTER burn-in\nnum_burnin_steps=100 # this is discarded (currently not other option in TPF)\n\nmysampler=tfp.mcmc.NoUTurnSampler(\n                                     target_log_prob_fn=log_prob_fn,\n                                     max_tree_depth=15, # default is 10\n                                     max_energy_diff=1000.0, # default do not change\n                                     step_size=steps_chains\n                                     )\n\nsampler = tfp.mcmc.TransformedTransitionKernel( # inside this the starting conditions must be on\n                                                # original scale i.e. precisions must be &gt;0\n    mysampler,\n    bijector=unconstraining_bijectors\n    )\n\n## define final sampler - NUTS, bijectors and adaptation\nadaptive_sampler = tfp.mcmc.DualAveragingStepSizeAdaptation(\n    inner_kernel=sampler,\n    num_adaptation_steps=int(0.8 * num_burnin_steps),\n    reduce_fn=tfp.math.reduce_logmeanexp, # default - this determines how to change the step\n                                          # adaptation across chains\n    #reduce_fn=tfp.math.reduce_log_harmonic_mean_exp, # might be better if difficult chains\n    target_accept_prob=tf.cast(0.95, tf.float32)) # this is a key parameter to get good mixing"
  },
  {
    "objectID": "tfp/intro_model_build.html#define-starting-point-for-chains",
    "href": "tfp/intro_model_build.html#define-starting-point-for-chains",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "Define starting point for chains",
    "text": "Define starting point for chains\nExplicit initial conditions are needed for each parameter in each chain. This is currently done very simply via hard coding. See RStan manual for a fairly simple way to generate random conditions that often works well. The key point here is that the initial conditions must be again in a specific tensor structure.\n\n\nCode\nistate=[tf.constant([0.0]),\n        tf.constant([0.5]),\n                   tf.constant([0.0]),\n        tf.constant([0.5]),\n        tf.constant([[1.,-1.]])]\n\ncurrent_state = [tf.expand_dims(tf.repeat(istate[0],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(istate[1],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(istate[2],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(istate[3],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.tile(istate[4],[n_chains,1]),axis=1) # starts with shape (1,2) i.e. [[a, b]]\n                 ]\nprint(current_state)\n\n\n[&lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[0.],\n       [0.]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[0.5],\n       [0.5]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[0.],\n       [0.]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[0.5],\n       [0.5]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(2, 1, 2), dtype=float32, numpy=\narray([[[ 1., -1.]],\n\n       [[ 1., -1.]]], dtype=float32)&gt;]\n\n\n\n\nCode\nprint(py$current_state)\n\n\n[[1]]\ntf.Tensor(\n[[0.]\n [0.]], shape=(2, 1), dtype=float32)\n\n[[2]]\ntf.Tensor(\n[[0.5]\n [0.5]], shape=(2, 1), dtype=float32)\n\n[[3]]\ntf.Tensor(\n[[0.]\n [0.]], shape=(2, 1), dtype=float32)\n\n[[4]]\ntf.Tensor(\n[[0.5]\n [0.5]], shape=(2, 1), dtype=float32)\n\n[[5]]\ntf.Tensor(\n[[[ 1. -1.]]\n\n [[ 1. -1.]]], shape=(2, 1, 2), dtype=float32)"
  },
  {
    "objectID": "tfp/intro_model_build.html#perform-mcmc-sampling",
    "href": "tfp/intro_model_build.html#perform-mcmc-sampling",
    "title": "Two-arm trial with No U-Turn sampling",
    "section": "Perform MCMC Sampling",
    "text": "Perform MCMC Sampling\nWe are now in a position to actually sample from our model, but first we will set up a tracer function which is called at every step. This can be used either for diagnostics, such as monitor step-size changes, or else to generate custom output, such as compute log-likelihood at each step. Such a function can be computationally expensive and so it’s a trade-off as to whether to wait until the samples have been generated to compute additional functions of the parameters of interest.\n\n\nCode\ndef trace_fn(state, pkr):\n    return {\n        'sample': state, # this is also pkr['all'][0].transformed_state[0]\n        'step_size': pkr.new_step_size,  # &lt;--- THIS extracts the adapted step size\n        'all': pkr, #state is also pkr['all'][0].transformed_state[0]\n        #pkr is a named tuple with ._fields = ('transformed_state', 'inner_results')\n        # 'transformed_state is the state\n        # 'inner_results' is diagnostics\n        # ('target_log_prob', 'grads_target_log_prob', 'step_size', 'log_accept_ratio', 'leapfrogs_taken',                     'is_accepted', 'reach_max_depth', 'has_divergence', 'energy', 'seed')\n        'has_divergence':pkr[0].inner_results.has_divergence,\n        'logL': log_prob_fn(state[0], state[1],state[2],state[3],state[4])\n    }\n\n\nNow run the actual sampler. The number of steps and burn-in were defined above when we setup the No U-Turn sampler.\n\n\nCode\n# Speed up sampling by tracing with `tf.function`.\n@tf.function(autograph=False, jit_compile=True,reduce_retracing=True)\ndef do_sampling():\n  return tfp.mcmc.sample_chain(\n      kernel=adaptive_sampler,\n      current_state=current_state,\n      num_results=num_results,\n      num_burnin_steps=num_burnin_steps,\n      seed= tf.constant([9999, 9999], dtype=tf.int32), # this is random seed\n      trace_fn=trace_fn)\n\n\nt0 = time.time()\n#samples, kernel_results = do_sampling()\nsamples, traceout = do_sampling()\n#res = do_sampling()\n#[mu0, sigma0, mu1, sigma1,log_odds_control_and_ratio], results = do_sampling()\nt1 = time.time()\nprint(\"Inference ran in {:.2f}s.\".format(t1-t0))\n\n\nInference ran in 36.35s.\n\n\nCode\n## there is a trailing dimension of 1 so need to squeeze to get each col is chain, and each row is parameter sample\nsamplesflat = list(map(lambda x: tf.squeeze(x).numpy(), samples))\nprint(f\" means for mu0, sigma0, mu1, sigma1\\n\")\n\n\n means for mu0, sigma0, mu1, sigma1\n\n\nCode\nthemeans=[np.mean(row) for row in samplesflat[0:4]]\n\n\n\n\nCode\n# this shows how the step size changes during adaptation\n#traceout['step_size']\n#traceout['step_size'][0][999] #step size for mu0 at iteration 999+1\n\n\n\n\nCode\nnames(py$themeans)&lt;-c(\"mu0\",\"sigma0\",\"mu1\",\"sigma1\")\nprint(py$themeans)\n\n\n$mu0\n[1] 0.3340306\n\n$sigma0\n[1] 1.561125\n\n$mu1\n[1] -0.1743813\n\n$sigma1\n[1] 1.609965\n\n\n\nSome Output plots\nWe plot the log-likelihood over the MCMC steps (post-burn-in) using two chains, a different colour for each chain. Similarly for trace plots. These are just illustrative output examples, the number of burn-in and total chain steps here are too low for reliable estimation.\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 5))\nplt.plot(tf.squeeze(traceout['logL'].numpy()))\nplt.title(\"log likelihood\")\n#plt.savefig(f\"precomputed/vg1_loglike.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, axes = plt.subplots(2, 2)#, sharex='col', sharey='col')\nfig.set_size_inches(10, 5)\naxes[0][0].plot(samplesflat[0]) # mu_b\naxes[0][1].plot(samplesflat[1]) # tau_b\naxes[1][0].plot(samplesflat[2]) # mu\naxes[1][1].plot(samplesflat[3]) # tau\n#plt.savefig(f\"precomputed/vg1_traces.png\")\nplt.show()"
  },
  {
    "objectID": "tfp/index_tfp.html",
    "href": "tfp/index_tfp.html",
    "title": "",
    "section": "",
    "text": "Code\nThis package is a collection of heavily documented Rmarkdown vignettes demonstrating how to use Tensorflow Probability (TFP) in RStudio for Bayesian inference. RStan features in many of these for comparison. The application focus is model building and trial simulation for clinical research and drug development."
  },
  {
    "objectID": "tfp/index_tfp.html#why-tensorflow-probability-tfp",
    "href": "tfp/index_tfp.html#why-tensorflow-probability-tfp",
    "title": "",
    "section": "Why Tensorflow Probability (TFP)?",
    "text": "Why Tensorflow Probability (TFP)?\n\nHaving alternative options, e.g., to RStan, when using open source reduces business risk\nTFP is developed and maintained by Google and is available globally\nTFP provides a wide variety of building blocks for probabilistic modelling and ML"
  },
  {
    "objectID": "tfp/index_tfp.html#running-the-vignettes",
    "href": "tfp/index_tfp.html#running-the-vignettes",
    "title": "",
    "section": "Running the Vignettes",
    "text": "Running the Vignettes\nLinks to the raw .Rmd files for download and local compilation can be found in each of the vignettes on this site. Local compilation has some dependencies which are given below.\n\nQuickstart\n\nInstall the tensorflow library and its install script must include “pandas” in the extra_packages option (see details below)\nInstall tfprobability\nEnsure pak is installed and loaded into session\nthen at R Console &gt; pak::pkg_install(\"fraseriainlewis/tfclinical\")\n\n\n\n\n\nInstallation details for Linux\nPython needs to be installed, and pyenv can also be a very useful tool if dealing with multiple Python versions and to avoid impacting the system Python installation.\n##########################################################\n# in RStudio Terminal (not R Console) or in Bash\nsudo apt-get update\nsudo apt-get install python3-venv python3-pip python3-dev\n##########################################################\n# in RStudio console\ninstall.packages(\"tensorflow\")\nlibrary(tensorflow)\ninstall_tensorflow(extra_packages = c(\"tf_keras\", \"tensorflow\", \"tensorflow-probability\",\"pandas\",\"matplotlib\"))\n# note - matplotlib is not essential, and may be removed or added later via py_install()\n#restart session\ninstall.packages(\"tfprobability\")\nlibrary(tensorflow)\nlibrary(tfprobability)\nd &lt;- tfd_binomial(total_count = 7, probs = 0.3) # if this works then tensorflow us correctly installed\n##########################################################\n\n\nInstallation details for Windows\nOn Windows the key part is to have a suitable Python installation and also that RStudio can locate this. Once this is in place then the installation process is as above for the Linux case.\n\n\nAdvanced: Installation of separate Python venv\nTo run the live Graphical Neural Network (GNN) vignette requires a separate Python venv to be setup because some tensorflow spin-off projects, such as TF-GNN, have strict compatibility requirements. Library versions likely need hardcoded as pip will not choose suitable combinations. The instructions below setup an appropriate venv for running the GNN vignette (only tested on MacOS).\n# in bash\npython3 -m venv gnn\nsource gnn/bin/activate\n# now install via pip the specific library versions needed\npip install tensorflow==2.16.2 tf_keras==2.16.0 tensorflow-gnn\nThe second part below in the R Console is to instruct reticulate to use this Python venv rather than one of the existing virtualenv environments, otherwise reticulate may use, e.g. the tensorflow venv created when doing the above install, which will likely not contain a compatible tensorflow version.\n# in RStudio console\nlibrary(reticulate)\nuse_virtualenv(\"/Users/work/gnn\", required = TRUE) # tell R to use the python interpreter and libraries in here"
  },
  {
    "objectID": "llm/index_llm.html",
    "href": "llm/index_llm.html",
    "title": "",
    "section": "",
    "text": "Code\nThis package is a collection of heavily documented Rmarkdown vignettes demonstrating how to use Tensorflow Probability (TFP) in RStudio for Bayesian inference. RStan features in many of these for comparison. The application focus is model building and trial simulation for clinical research and drug development."
  },
  {
    "objectID": "llm/index_llm.html#why-tensorflow-probability-tfp",
    "href": "llm/index_llm.html#why-tensorflow-probability-tfp",
    "title": "",
    "section": "Why Tensorflow Probability (TFP)?",
    "text": "Why Tensorflow Probability (TFP)?\n\nHaving alternative options, e.g., to RStan, when using open source reduces business risk\nTFP is developed and maintained by Google and is available globally\nTFP provides a wide variety of building blocks for probabilistic modelling and ML"
  },
  {
    "objectID": "llm/index_llm.html#running-the-vignettes",
    "href": "llm/index_llm.html#running-the-vignettes",
    "title": "",
    "section": "Running the Vignettes",
    "text": "Running the Vignettes\nLinks to the raw .Rmd files for download and local compilation can be found in each of the vignettes on this site. Local compilation has some dependencies which are given below.\n\nQuickstart\n\nInstall the tensorflow library and its install script must include “pandas” in the extra_packages option (see details below)\nInstall tfprobability\nEnsure pak is installed and loaded into session\nthen at R Console &gt; pak::pkg_install(\"fraseriainlewis/tfclinical\")\n\n\n\n\n\nInstallation details for Linux\nPython needs to be installed, and pyenv can also be a very useful tool if dealing with multiple Python versions and to avoid impacting the system Python installation.\n##########################################################\n# in RStudio Terminal (not R Console) or in Bash\nsudo apt-get update\nsudo apt-get install python3-venv python3-pip python3-dev\n##########################################################\n# in RStudio console\ninstall.packages(\"tensorflow\")\nlibrary(tensorflow)\ninstall_tensorflow(extra_packages = c(\"tf_keras\", \"tensorflow\", \"tensorflow-probability\",\"pandas\",\"matplotlib\"))\n# note - matplotlib is not essential, and may be removed or added later via py_install()\n#restart session\ninstall.packages(\"tfprobability\")\nlibrary(tensorflow)\nlibrary(tfprobability)\nd &lt;- tfd_binomial(total_count = 7, probs = 0.3) # if this works then tensorflow us correctly installed\n##########################################################\n\n\nInstallation details for Windows\nOn Windows the key part is to have a suitable Python installation and also that RStudio can locate this. Once this is in place then the installation process is as above for the Linux case.\n\n\nAdvanced: Installation of separate Python venv\nTo run the live Graphical Neural Network (GNN) vignette requires a separate Python venv to be setup because some tensorflow spin-off projects, such as TF-GNN, have strict compatibility requirements. Library versions likely need hardcoded as pip will not choose suitable combinations. The instructions below setup an appropriate venv for running the GNN vignette (only tested on MacOS).\n# in bash\npython3 -m venv gnn\nsource gnn/bin/activate\n# now install via pip the specific library versions needed\npip install tensorflow==2.16.2 tf_keras==2.16.0 tensorflow-gnn\nThe second part below in the R Console is to instruct reticulate to use this Python venv rather than one of the existing virtualenv environments, otherwise reticulate may use, e.g. the tensorflow venv created when doing the above install, which will likely not contain a compatible tensorflow version.\n# in RStudio console\nlibrary(reticulate)\nuse_virtualenv(\"/Users/work/gnn\", required = TRUE) # tell R to use the python interpreter and libraries in here"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\nCode\n1 + 1\n\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This site contains a collection of R vignettes covering a range of themes relevant to statisticians working in drug development and clinical research. While we hope these vignettes are a useful resource, they are provided ‘as is’ without warranty of any kind, express or implied and used at your own risk."
  },
  {
    "objectID": "index.html#key-themes-and-topics",
    "href": "index.html#key-themes-and-topics",
    "title": "Home",
    "section": "Key Themes and Topics",
    "text": "Key Themes and Topics\n\nTensorflow Probability\n\nTensorflow Probability is a relatively new library for Bayesian modelling building on the Tensorflow computational framework for large scale computation. An alternative to RStan and removes reliance on a single library for business critical modelling\n\nGraphical Neural Networks\n\nGraphical Neural Networks (GNN) are new tools and the go-to option for modelling in certain drug discovery tasks, such as properties of new molecules. This is a general methodology with wide application across clinical research for prediction and clustering.\n\nLLM prompts for statistical work\n\nLarge language models are changing the way statisticians work. Effectively using LLMs is a skill on its own, especially how to design and iterate prompts to LLMs save effort overall."
  },
  {
    "objectID": "ml/ml_gnn_example1.html",
    "href": "ml/ml_gnn_example1.html",
    "title": "tf_gnn_example",
    "section": "",
    "text": "Code\n#library(tfclinical)\nlibrary(ragg)\nlibrary(reticulate)\nuse_virtualenv(\"/Users/work/gnn\", required = TRUE)\n# 1. Set the device to 'png' (which both R and Python understand)\nknitr::opts_chunk$set(dev = \"png\")\n\n# 2. Tell R to use ragg as the default engine for all png devices\noptions(device = function(...) ragg::agg_png(...))"
  },
  {
    "objectID": "ml/ml_gnn_example1.html#graphical-neural-network-model",
    "href": "ml/ml_gnn_example1.html#graphical-neural-network-model",
    "title": "tf_gnn_example",
    "section": "Graphical Neural Network Model",
    "text": "Graphical Neural Network Model\nGraphical Neural Networks are increasingly finding application in drug development beyond their historical major use case of prediction and classification of new potential drug molecules. Newer applications include extending to electronic patient records for prediction of future disease outcomes.\nThis example is ported from the original on Tensorflow GNN github site. Unlike other vignettes, a custom Python setup is required here for reticulate so as ensure correct version compatibility of tensorflow and tensorflow-gnn. See installation guide under advanced.\nNeeds compatible versions (installed via pip in venv hard coding versions):\n\nPython: 3.11.14\nName: tf_keras\n\nVersion: 2.16.0\n\nName: tensorflow\n\nVersion: 2.16.2\n\nName: tensorflow-gnn\n\nVersion: 1.0.3\n\n\n\n\nCode\nimport os\nos.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"  # For TF2.16+.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_gnn as tfgnn\n\nprint(f'Running TF-GNN {tfgnn.__version__} with TensorFlow {tf.__version__}.')\n\n\n\n\ntrain_path = os.path.join(os.getcwd(), 'precomputed/mutag', 'train.tfrecords')\nval_path = os.path.join(os.getcwd(), 'precomputed/mutag', 'val.tfrecords')\n#get_ipython().system('ls -l {train_path} {val_path}')\n\nprint(f\" the train path={train_path}\")\n#exit()\n\n\ngraph_tensor_spec = tfgnn.GraphTensorSpec.from_piece_specs(\n    context_spec=tfgnn.ContextSpec.from_field_specs(features_spec={\n                  'label': tf.TensorSpec(shape=(1,), dtype=tf.int32)\n    }),\n    node_sets_spec={\n        'atoms':\n            tfgnn.NodeSetSpec.from_field_specs(\n                features_spec={\n                    tfgnn.HIDDEN_STATE:\n                        tf.TensorSpec((None, 7), tf.float32)\n                },\n                sizes_spec=tf.TensorSpec((1,), tf.int32))\n    },\n    edge_sets_spec={\n        'bonds':\n            tfgnn.EdgeSetSpec.from_field_specs(\n                features_spec={\n                    tfgnn.HIDDEN_STATE:\n                        tf.TensorSpec((None, 4), tf.float32)\n                },\n                sizes_spec=tf.TensorSpec((1,), tf.int32),\n                adjacency_spec=tfgnn.AdjacencySpec.from_incident_node_sets(\n                    'atoms', 'atoms'))\n    })\n\n\ndef decode_fn(record_bytes):\n  graph = tfgnn.parse_single_example(\n      graph_tensor_spec, record_bytes, validate=True)\n\n  # extract label from context and remove from input graph\n  context_features = graph.context.get_features_dict()\n  label = context_features.pop('label')\n  new_graph = graph.replace_features(context=context_features)\n\n  return new_graph, label\n\n\n# In[7]:\n\n\ntrain_ds = tf.data.TFRecordDataset([train_path]).map(decode_fn)\nval_ds = tf.data.TFRecordDataset([val_path]).map(decode_fn)\n\n\n# ### Look at one example from the dataset\n\n# In[8]:\n\n\ng, y = train_ds.take(1).get_single_element()\n\n\n# #### Node features\n# \n# Node features represent the 1-hot encoding of the atom type (0=C, 1=N, 2=O, 3=F,\n# 4=I, 5=Cl, 6=Br).\n\n# In[9]:\n\n\nprint(g.node_sets['atoms'].features[tfgnn.HIDDEN_STATE])\n\n\n# #### Bond Edges\n# \n# In this example, we consider the bonds between atoms undirected edges. To encode\n# them in the GraphsTuple, we store the undirected edges as pairs of directed\n# edges in both directions.\n# \n# `adjacency.source` contains the source node indices, and `adjacency.target` contains the corresponding target node indices.\n\n# In[10]:\n\n\ng.edge_sets['bonds'].adjacency.source\n\n\n# In[11]:\n\n\ng.edge_sets['bonds'].adjacency.target\n\n\n# #### Edge features\n# \n# Edge features represent the bond type as one-hot encoding.\n\n# In[12]:\n\n\ng.edge_sets['bonds'].features[tfgnn.HIDDEN_STATE]\n\n\n# ### Label\n# The label is binary, indicating the mutagenicity of the molecule. It's either 0 or 1.\n\n# In[13]:\n\n\ny\n\n\n\n\nCode\n#for k, hist in history.history.items():\n#  plt.plot(hist)\n#  plt.title(k)\n#  plt.show()\nfor k, hist in history.history.items():\n    plt.figure()  # Create a new figure for each metric\n    plt.plot(hist)\n    plt.title(k)\n    plt.xlabel('Epoch')\n    plt.ylabel(k)\n    \n    # Save the plot. Using f-strings to name the file based on the key (e.g., loss.png)\n    #plt.savefig(f\"precomputed/pyplot_{k}.png\")\n    \n    # Optional: If you want to show it in the console while running\n    # plt.show() \n    \n    #plt.close() \n    \n\n# Feel free to play with the hyperparameters and the model architecture to improve the results!"
  },
  {
    "objectID": "ml/ml_gnn_example1.html#the-end",
    "href": "ml/ml_gnn_example1.html#the-end",
    "title": "tf_gnn_example",
    "section": "the end",
    "text": "the end"
  },
  {
    "objectID": "tfp/intro_2armstan.html",
    "href": "tfp/intro_2armstan.html",
    "title": "Two-arm trial TFP v RStan",
    "section": "",
    "text": "This vignette uses the same model as in “Two-arm trial with No U-Turn sampling” but now includes a numerical comparison with the same model implemented in RStan. The code chunks here for the TFP model are more compact and slightly re-ordered and abbreviated. The takeaway is that both libraries give similar results, although some tweaking may be required to TFP, e.g. in the target acceptance probability.\nOne remark is that RStan readily reports diagnostics such as divergent transitions. Similar dignostic output can be captured in TFP via the trace callback (e.g. see the trace fn used below which also captures divergent transitions). RStan has many other diagnostic capabilities such as ESS_bulk and ESS_tail sample sizes which can be readily applied to sampler output from TFP as reticulate makes the samples available from R.\nClick here for the full R Markdown file that generated all the outputs shown here.\n\n\nThe model implemented here is a non-centered parameterization for a logistic model.\n\\[\n\\begin{aligned}\n\\mu_0 &\\sim \\text{Normal}(0, 2.5)\\\\\n\\sigma_0 &\\sim \\text{Half-Normal}(0, 2.5) \\\\\n\\mu_1 &\\sim \\text{Normal}(0, 2.5) \\\\\n\\sigma_1 &\\sim \\text{Half-Normal}(0, 2.5) \\\\\n\\tilde{\\theta}_j &\\sim \\text{Normal}(0, 1)\\quad\\enspace\\text{for}\\enspace{j=1,2}  \\\\\n\\beta_0 &= \\mu_0 + \\sigma_0 \\tilde{\\theta}_1 \\\\\n\\beta_1 &= \\mu_1 + \\sigma_1 \\tilde{\\theta}_2 \\\\\n\\text{logit(}{p_i}) &= \\beta_0 + \\beta_1 z_i\\quad\\quad\\enspace\\enspace\\text{for}\\enspace{i=1,\\dots,N}  \\\\\ny_i &\\sim \\text{Bernoulli}(p_i)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "tfp/intro_2armstan.html#quick-start",
    "href": "tfp/intro_2armstan.html#quick-start",
    "title": "Two-arm trial TFP v RStan",
    "section": "",
    "text": "This vignette uses the same model as in “Two-arm trial with No U-Turn sampling” but now includes a numerical comparison with the same model implemented in RStan. The code chunks here for the TFP model are more compact and slightly re-ordered and abbreviated. The takeaway is that both libraries give similar results, although some tweaking may be required to TFP, e.g. in the target acceptance probability.\nOne remark is that RStan readily reports diagnostics such as divergent transitions. Similar dignostic output can be captured in TFP via the trace callback (e.g. see the trace fn used below which also captures divergent transitions). RStan has many other diagnostic capabilities such as ESS_bulk and ESS_tail sample sizes which can be readily applied to sampler output from TFP as reticulate makes the samples available from R.\nClick here for the full R Markdown file that generated all the outputs shown here.\n\n\nThe model implemented here is a non-centered parameterization for a logistic model.\n\\[\n\\begin{aligned}\n\\mu_0 &\\sim \\text{Normal}(0, 2.5)\\\\\n\\sigma_0 &\\sim \\text{Half-Normal}(0, 2.5) \\\\\n\\mu_1 &\\sim \\text{Normal}(0, 2.5) \\\\\n\\sigma_1 &\\sim \\text{Half-Normal}(0, 2.5) \\\\\n\\tilde{\\theta}_j &\\sim \\text{Normal}(0, 1)\\quad\\enspace\\text{for}\\enspace{j=1,2}  \\\\\n\\beta_0 &= \\mu_0 + \\sigma_0 \\tilde{\\theta}_1 \\\\\n\\beta_1 &= \\mu_1 + \\sigma_1 \\tilde{\\theta}_2 \\\\\n\\text{logit(}{p_i}) &= \\beta_0 + \\beta_1 z_i\\quad\\quad\\enspace\\enspace\\text{for}\\enspace{i=1,\\dots,N}  \\\\\ny_i &\\sim \\text{Bernoulli}(p_i)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "tfp/intro_2armstan.html#data-set",
    "href": "tfp/intro_2armstan.html#data-set",
    "title": "Two-arm trial TFP v RStan",
    "section": "Data set",
    "text": "Data set\nThe R chunk below creates a simple dataset of three cols:\n\na binary response variable (0/1 = non-responder/responder),\na binary treatment variable (0/1 = control/test treatment)\nand a basket ID variable. (1)\n\nThe basket ID is currently set fixed at 1, denoting there is only one basket in this trial, i.e. a classical two arm randomized trial design. Baskets will be added in other vignettes.\n\n\nCode\n### Prepare model inputs\nset.seed(99999)\n# Set up data\nrr_k_ctrl &lt;- c(0.60)        # control response rate for each basket\nrr_k_trt &lt;- c(0.58)         # treatment response rate for each basket\n\nK&lt;-length(rr_k_ctrl)        # number of baskets\n\nN_k_ctrl &lt;- rep(250, K)     # number of control participants per basket\nN_k_trt &lt;- rep(250, K)      # number of treatment participants per basket\nN_k &lt;- N_k_ctrl + N_k_trt   # number of participants per basket (both arms combined)\nN &lt;- sum(N_k)               # total sample size\nk_vec &lt;- rep(1:K, N_k)      # N x 1 vector of basket indicators (1 to K)\n\nz_vec&lt;-NULL;\ny&lt;-NULL;\nfor(i in 1:K){ # for each basket repeat 0-control 1-trt according to the specifc Ns\n  z_vec&lt;-c(z_vec,rep(0:1,c(N_k_ctrl[i],N_k_trt[i]))) # treatment/control indicator\n  y&lt;-c(y,\n       c(rbinom(N_k_ctrl[i],1,rr_k_ctrl[i]), # bernoulli for control\n         rbinom(N_k_trt[i],1,rr_k_trt[i]))) #           for trt\n}\n\nthedata&lt;-data.frame(y,basketID=k_vec,Treatment=z_vec)\n\npy$thedata&lt;-r_to_py(thedata) # THE KEY LINE - makes data available to Python\n                             # this is converted into a pandas dataframe object in Python"
  },
  {
    "objectID": "tfp/intro_2armstan.html#rstan",
    "href": "tfp/intro_2armstan.html#rstan",
    "title": "Two-arm trial TFP v RStan",
    "section": "RStan",
    "text": "RStan\nThe RStan workflow is somewhat more compact than the TFP equivalent, where TFP requires more lower level details to be specified whereas these are dealt with behind the scences in RStan. ### Model definition This is somewhat simplified for ease of use, e.g. hard coded hyperparameters.\n\n\nCode\n### Stan model\n  BHM_stan_1 &lt;- \"data {\n  int&lt;lower = 0&gt; K;                     // number of baskets (K &gt;= 2)\n  int&lt;lower = 0&gt; N;                     // total number of participants\n  //int&lt;lower = 0, upper = N&gt; N_k[K];     // K x 1 vector of basket sample sizes\n  int&lt;lower = 0, upper = N&gt; N_k;\n  int&lt;lower = 1, upper = K&gt; k_vec[N];   // N x 1 vector of basket indicators\n  int&lt;lower = 0, upper = 1&gt; z_vec[N];   // N x 1 vector of treatment indicators for active treatment arm\n  int&lt;lower = 0, upper = 1&gt; y[N];       // N x 1 vector of binary responses\n}\nparameters {\n  matrix[K,2] beta_tr;                  // K x 2 matrix of transformed regression coefficients\n  real mu1;                              // scalar of hierarchical mean (log odds ratio)\n  real&lt;lower = 0&gt; sigma1;                  // scalar of hierarchical SD (log odds ratio)\n  real mu0;                              // scalar of hierarchical mean (log odds ratio)\n  real&lt;lower = 0&gt; sigma0;                  // scalar of hierarchical SD (log odds ratio)\n\n}\ntransformed parameters {\n  matrix[K,2] beta;                     // K x 2 matrix of regression coefficients (without transformation)\n  // Calculate beta coefs using transformed betas (leads to better mixing)\n  for (k in 1:K){\n    beta[k,1] = mu0 + sigma0 * beta_tr[k,1];\n    beta[k,2] = mu1 + sigma1 * beta_tr[k,2];\n  }\n\n}\nmodel {\n  mu0 ~ normal(0, 2.5);    // vectorized normal priors for hierarchical means (specify SD in normal distn)\n  sigma0 ~ normal(0, 2.5);       // vectorized half-normal priors for hierarchical SDs (specify SD in normal distn)\n  mu1 ~ normal(0, 2.5);    // vectorized normal priors for hierarchical means (specify SD in normal distn)\n  sigma1 ~ normal(0, 2.5);       // vectorized half-normal priors for hierarchical SDs (specify SD in normal distn)\n  beta_tr[,1] ~ normal(0, 1);  // vectorized normal prior for the log odds for the control arm\n  beta_tr[,2] ~ normal(0, 1);  // vectorized normal prior for the log odds ratios\n  for (i in 1:N)\n    y[i] ~ bernoulli_logit(beta[k_vec[i], 1] + beta[k_vec[i], 2] * z_vec[i]);\n}\n\"\n\n\n\nSetup and run sampler\n\n\nCode\n\n# Create list with input values for Stan model\ndata_input_1 &lt;- list(\n  K = K,                # number of subgroups\n  N = N,                # total sample size\n  N_k = N_k,            # sample size per basket\n  k_vec = k_vec,        # N x 1 vector of subgroup indicators\n  z_vec = z_vec,        # N x 1 vector of active treatment arm indicators\n  y = y                # N x 1 vector of binary responses\n  )\n\n\n\n### Compile and fit model\noptions(mc.cores = parallel::detectCores())\n# Compile model (only run the line below once for a simulation study - compilation not dependent on any\n# simulation inputs as defined by scenarios or on simulated data)\nstan_mod_1 &lt;- stan_model(model_code = BHM_stan_1)\n# Fit model\nnsamps &lt;- 10000       # number of posterior samples (after removing burn-in) per chain\nnburnin &lt;- 1000       # number of burn-in samples to remove at beginning of each chain\nnchains &lt;- 4          # number of chains\nBHM_pars_1 &lt;- c(\"mu0\", \"sigma0\",\"mu1\", \"sigma1\", \"beta\",\"beta_tr\")     # parameters to sample\nstart_time &lt;- Sys.time()\nstan_fit_2 &lt;- sampling(stan_mod_1, data = data_input_1, pars = BHM_pars_1,\n                       iter = nsamps + nburnin, warmup = nburnin, chains = nchains)\n#&gt; Warning: There were 109 divergent transitions after warmup. See\n#&gt; https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n#&gt; to find out why this is a problem and how to eliminate them.\n#&gt; Warning: Examine the pairs() plot to diagnose sampling problems\nend_time &lt;- Sys.time()\nend_time - start_time\npost_draws_2 &lt;- as.matrix(stan_fit_2)           # posterior samples of each parameter\n\n\n\n\nSummarize results\n\n\nCode\nlibrary(bayesplot)\ncolor_scheme_set(\"viridisA\")\np&lt;-mcmc_trace(stan_fit_2,\"mu0\")\n#ggsave(\"precomputed/vg2_traceplot1_stan.png\", p)\nprint(p)"
  },
  {
    "objectID": "tfp/intro_2armstan.html#now-for-the-tfp-equivalent-model",
    "href": "tfp/intro_2armstan.html#now-for-the-tfp-equivalent-model",
    "title": "Two-arm trial TFP v RStan",
    "section": "Now for the TFP Equivalent Model",
    "text": "Now for the TFP Equivalent Model\n\nSetup\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport os\nimport keras\n#&gt; /Users/work/tfp/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n#&gt;   if not hasattr(np, \"object\"):\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow_probability import distributions as tfd\ntfb = tfp.bijectors\nimport warnings\nimport time\nimport sys\n\n## The data.frame passed is now a panda df but for TPF this needs to be further\n## converted into tensors here we simply convert each col of the data.frame into\n## a Rank 1 tensor (i.e. a vector)\ny_data=tf.convert_to_tensor(thedata.iloc[:,0], dtype = tf.float32)\nk_vec=tf.convert_to_tensor(thedata.iloc[:,1], dtype = tf.float32)\nz_vec=tf.convert_to_tensor(thedata.iloc[:,2], dtype = tf.float32)\n\n\n\n\nModel specification\n\n\nCode\n# Define LIKELIHOOD - this is defined first, as functions need defined before used\n# The arguments in our function are in the REVERSE ORDER that the parameters\n# appear in JointDistrbutionSequential this is essential\ndef make_observed_dist(log_odd_control_and_ratio,sigma1, mu1,sigma0,mu0):\n    # beta is a two element tensor, beta[0] parameter for the log odds of the control arm effect\n    #                               beta[1] log odds ratio of treatment to control\n    beta=tf.stack([mu0 + sigma0 * log_odd_control_and_ratio[0],\n                   mu1 + sigma1 * log_odd_control_and_ratio[1]])\n                   #tf.stack is used to stack tensors without creating new tensors\n\n    # return a vector of Bernoulli probabilities, one for each patient, and these estimates\n    # dependent on which arm the patient was randomized to, i.e. two unique values\n    return(tfd.Independent(\n        tfd.Bernoulli(logits=beta[0]+beta[1]*z_vec), #define as logits\n        reinterpreted_batch_ndims = 1 # This tells TFP that log_prob here is a single value\n                                      # equal to sum of individual log_probs, a vector\n                                      # this is usual when defining a data likelihood\n    ))\n    \n    \n# DEFINE FULL MODEL - hyperparams hard coded\n# model is y[i] = Bernoulli(p[i]) where logit(p[i]) = intercept + treatment*z[i]\nmodel = tfd.JointDistributionSequentialAutoBatched([\n  tfd.Normal(loc=0., scale=2.5, name=\"mu0\"),  # prior for mean of control arm log odds\n  tfd.HalfNormal(scale=2.5, name=\"sigma0\"),   # prior for sd of control arm log odds\n  tfd.Normal(loc=0., scale=2.5, name=\"mu1\"),  # prior for mean of treatment log odds ratio\n  tfd.HalfNormal(scale=2.5, name=\"sigma1\"),   # prior for sd of treatment log odds ratio\n  # now come to the standard normals resulting from non-centred parameterization\n  # define these as vector MVN with identity scale matrix\n  tfd.Normal(loc=tf.zeros(2), scale=tf.ones(2), name=\"log_odd_control_and_ratio\"),\n  # now finally define the likelihood as we have already defined parameters in this\n  make_observed_dist # the data likelihood defined as a function above\n])    \n\n# the ordering of the arguments in this must match exactly the ordering used in\n# JointDistributionSequentialAutoBatched\ndef log_prob_fn(mu0, sigma0, mu1,sigma1, log_odd_control_and_ratio):\n  return model.log_prob((\n      mu0, sigma0, mu1,sigma1, log_odd_control_and_ratio,y_data))\n                                                         ## last argument is the DATA (y)\n    \n\n\n\n\nSetup the MCMC sampling\nFirst the initial step sizes and initial conditions\n\n\nCode\n\n## Adaptive step size.\n## Do in two parts, first a structure to allow step size to adapt separately for\n## each parameter\nsteps=[tf.constant([0.5]),                # mu0\n        tf.constant([0.05]),              # sigma0\n        tf.constant([0.5]),               # mu1\n        tf.constant([0.05]),              # sigma1\n        tf.constant([[0.5,0.5]]) # log_odd_control_and_ratio\n                                 # NOTE - vector and must have correct shape\n        ]\n\n## now we replicate the above \"steps\" array into a structure where this is\n## repeated inside each chain\nn_chains=4 ## THIS SETS NUMBER OF CHAINS\nsteps_chains = [tf.expand_dims(tf.repeat(steps[0],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(steps[1],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(steps[2],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(steps[3],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.tile(steps[4],[n_chains,1]),axis=1) # starts with shape (1,2) i.e. [[a, b]]\n                 ]\n                 \n# initial conditions\nistate=[tf.constant([0.0]),\n        tf.constant([0.5]),\n                   tf.constant([0.0]),\n        tf.constant([0.5]),\n        tf.constant([[1.,-1.]])]\n\ncurrent_state = [tf.expand_dims(tf.repeat(istate[0],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(istate[1],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(istate[2],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(istate[3],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.tile(istate[4],[n_chains,1]),axis=1) # starts with shape (1,2) i.e. [[a, b]]\n                 ]\n                 \n\n\nNow the sampler structure, bijectors, step size adaptation and the technical parameters for No U-Turn.\n\n\nCode\n# bijectors which define the mapping from **unconstrained space to target space**\n# i.e. Exp() is used for scale as this maps R -&gt; R+\nunconstraining_bijectors = [\n    tfb.Identity(),  # mu0\n    tfb.Exp(),       # sigma0\n    tfb.Identity(),  # mu1\n    tfb.Exp(),       # sigma0\n    tfb.Identity()  # log_odd_control_and_ratio - this is a vector parameter\n]\n\nnum_results=10000 # number of steps to run each chain for AFTER burn-in\nnum_burnin_steps=1000 # this is discarded (currently not other option in TPF)\n\nmysampler=tfp.mcmc.NoUTurnSampler(\n                                     target_log_prob_fn=log_prob_fn,\n                                     max_tree_depth=15, # default is 10\n                                     max_energy_diff=1000.0, # default do not change\n                                     step_size=steps_chains\n                                     )\n\nsampler = tfp.mcmc.TransformedTransitionKernel( # inside this the starting conditions must be on\n                                                # original scale i.e. precisions must be &gt;0\n    mysampler,\n    bijector=unconstraining_bijectors\n    )\n\n## define final sampler - NUTS, bijectors and adaptation\nadaptive_sampler = tfp.mcmc.DualAveragingStepSizeAdaptation(\n    inner_kernel=sampler,\n    num_adaptation_steps=int(0.8 * num_burnin_steps),\n    reduce_fn=tfp.math.reduce_logmeanexp, # default - this determines how to change the step\n                                          # adaptation across chains\n    #reduce_fn=tfp.math.reduce_log_harmonic_mean_exp, # might be better if difficult chains\n    target_accept_prob=tf.cast(0.95, tf.float32)) # this is a key parameter to get good mixing\n\n\nNow define the trace/monitoring function.\n\n\nCode\ndef trace_fn(state, pkr):\n    return {\n        'sample': state, # this is also pkr['all'][0].transformed_state[0]\n        'step_size': pkr.new_step_size,  # &lt;--- THIS extracts the adapted step size\n        'all': pkr, #state is also pkr['all'][0].transformed_state[0]\n        #pkr is a named tuple with ._fields = ('transformed_state', 'inner_results')\n        # 'transformed_state is the state\n        # 'inner_results' is diagnostics\n        # ('target_log_prob', 'grads_target_log_prob', 'step_size', 'log_accept_ratio', 'leapfrogs_taken',                     'is_accepted', 'reach_max_depth', 'has_divergence', 'energy', 'seed')\n        'has_divergence':pkr[0].inner_results.has_divergence,\n        'logL': log_prob_fn(state[0], state[1],state[2],state[3],state[4])\n    }\n\n\n\n\nRun the actual sampler\nThe number of steps and burn-in were defined above when we setup the No U-Turn sampler.\n\n\nCode\n# Speed up sampling by tracing with `tf.function`.\n@tf.function(autograph=False, jit_compile=True,reduce_retracing=True)\ndef do_sampling():\n  return tfp.mcmc.sample_chain(\n      kernel=adaptive_sampler,\n      current_state=current_state,\n      num_results=num_results,\n      num_burnin_steps=num_burnin_steps,\n      seed= tf.constant([9999, 9999], dtype=tf.int32), # this is random seed\n      trace_fn=trace_fn)\n\n\nt0 = time.time()\n#samples, kernel_results = do_sampling()\nsamples, traceout = do_sampling()\n#res = do_sampling()\n#[mu0, sigma0, mu1, sigma1,log_odds_control_and_ratio], results = do_sampling()\nt1 = time.time()\nprint(\"Inference ran in {:.2f}s.\".format(t1-t0))\n#&gt; Inference ran in 166.91s.\n\n## there is a trailing dimension of 1 so need to squeeze to get each col is chain, and each row is parameter sample\nsamplesflat = list(map(lambda x: tf.squeeze(x).numpy(), samples))\n\n\n\n\nSome Output plots\nWe plot the log-likelihood over the MCMC steps (post-burn-in) using two chains, a different colour for each chain. Similarly for trace plots. These are just illustrative output examples, the number of burn-in and total chain steps here are too low for reliable estimation.\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nplt.figure(figsize=(10, 5))\nplt.plot(samplesflat[0]) # mu_b\n#plt.savefig(f\"precomputed/vg2_traces.png\")\nplt.show()"
  },
  {
    "objectID": "tfp/intro_neg_bin_tfp_stan.html",
    "href": "tfp/intro_neg_bin_tfp_stan.html",
    "title": "neg_bin_tfp_stan",
    "section": "",
    "text": "Code\n### Prepare model inputs\nset.seed(9999)\nlibrary(rstanarm)\ndata(roaches)\nroaches_cp&lt;-roaches # will make manual edits \nroaches_cp$roach1&lt;-roaches_cp$roach1/100;# manual\nroaches_cp$exposure2&lt;-log(roaches_cp$exposure2) # exposure is logged\npy$data&lt;-r_to_py(roaches_cp)\n\n\n\n\nCode\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow_probability import distributions as tfd\ntfb = tfp.bijectors\nimport numpy as np\nimport pandas as pd\nimport time\n\nrows, columns = data.shape\ny_data=tf.convert_to_tensor(data.iloc[:,0], dtype = tf.float32)\nX=tf.convert_to_tensor(data.iloc[:,1:],dtype=tf.float32)\nX=tf.concat([tf.ones([rows,1],dtype=tf.float32), X], axis=1)\n\n# observe lambda*t = a + b + c but want just lambda, i.e. Y=lambda*t so lambda = Y/t\n# log(lambda) = a + b + c  + log(exposure)\n# P(X=x) = lambda^x exp(-lambda)/x!  lambda = lambda2*t\n#\n\nbeta_expos=tf.convert_to_tensor(1.0,dtype=tf.float32) # dummy\n\ndef make_observed_dist(phi, beta_senior,beta_treatment, beta_roach,alpha):\n    \"\"\"Function to create the observed Normal distribution.\"\"\"\n    \n    B=tf.stack([alpha,beta_roach,beta_treatment,beta_senior,beta_expos])\n    #print(B._shape_as_list())\n    logmu=tf.linalg.matvec(X,B)\n\n    phi_expanded = tf.expand_dims(phi, -1)  # (3, 1)\n    mu = tf.math.exp(logmu)\n    #r = tf$expand_dims(1.0, -1L)/ phi_expanded;  # total_count = 5\n    #probs &lt;- r / (r + mu)\n\n    prob = phi_expanded/(phi_expanded+mu)\n    #prob&lt;-(phi_expanded)/(mu+phi_expanded)\n    # Create distribution for observations\n    return(tfd.Independent(\n        #tfd_normal(loc = mu, scale = sigma_expanded),\n        #mu = exp(mu)\n        #phi &lt;- 0.2  # scale/overdispersion\n\n        #r = tf$expand_dims(1.0, -1L)/ sigma_expanded;  # total_count = 5\n        #probs &lt;- r / (r + mu)\n        tfd.NegativeBinomial(total_count = phi_expanded, probs = 1-prob),\n        reinterpreted_batch_ndims = 1\n    ))\n\n## -------- this needs contructed via cat\n# Define the joint distribution without matrix mult\nmodel = tfd.JointDistributionSequentialAutoBatched([\n  tfd.Normal(loc=0., scale=5., name=\"alpha\"),  # # Intercept (alpha)\n  tfd.Normal(loc=0., scale=2.5, name=\"beta_roach\"),  # # Slope (beta_roach1)\n  tfd.Normal(loc=0., scale=2.5, name=\"beta_treatment\"),  # # Intercept (alpha)\n  tfd.Normal(loc=0., scale=2.5, name=\"beta_senior\"),  # # Slope (beta_roach1)\n  tfd.Exponential(rate=1., name=\"phi\"),\n  make_observed_dist\n])\n\ntf.random.set_seed(99990)\n\ndef log_prob_fn(alpha, beta_roach,beta_treatment,beta_senior,phi):\n  \"\"\"Unnormalized target density as a function of states.\"\"\"\n  return model.log_prob((\n      alpha, beta_roach,beta_treatment,beta_senior,phi, y_data))\n\ndef neg_log_prob_fn(pars):\n    alpha=pars[[0]]\n    beta_roach=pars[[1]]\n    beta_treatment=pars[[2]]\n    beta_senior=pars[[3]]\n    phi=pars[[4]]\n    \"\"\"Unnormalized target density as a function of states.\"\"\"\n    return -model.log_prob((\n      alpha, beta_roach,beta_treatment,beta_senior,phi, y_data))\n\n\n#### get starting values by find MLE\nif(True):\n    start = tf.constant([0.1,0.1,0.1,0.1,0.1],dtype = tf.float32)  # Starting point for the search.\n    optim_results = tfp.optimizer.nelder_mead_minimize(neg_log_prob_fn,\n                 initial_vertex=start, func_tolerance=1e-04,max_iterations=1000)\n\n    #print(optim_results.initial_objective_values)\n    #print(optim_results.objective_value)\n    #print(optim_results.position)\n  \n# bijector to map contrained parameters to real\nunconstraining_bijectors = [\n    tfb.Identity(),\n    tfb.Identity(),\n    tfb.Identity(),\n    tfb.Identity(),\n    tfb.Exp()\n]\n\nnum_results=20000\nnum_burnin_steps=10000\n\nsampler = tfp.mcmc.TransformedTransitionKernel(\n    tfp.mcmc.NoUTurnSampler(\n        target_log_prob_fn=log_prob_fn,\n        step_size=tf.cast(0.5, tf.float32)), #tf.cast(0.1, tf.float32)),\n    bijector=unconstraining_bijectors\n    )\n\nadaptive_sampler = tfp.mcmc.DualAveragingStepSizeAdaptation(\n    inner_kernel=sampler,\n    num_adaptation_steps=int(0.8 * num_burnin_steps),\n    target_accept_prob=tf.cast(0.8, tf.float32))\n\nistate = optim_results.position\n\nn_chains=3\ncurrent_state = [tf.expand_dims(tf.repeat(istate[0],repeats=n_chains,axis=-1),axis=-1),\n                 tf.expand_dims(tf.repeat(istate[1],repeats=n_chains,axis=-1),axis=-1),\n                 tf.expand_dims(tf.repeat(istate[2],repeats=n_chains,axis=-1),axis=-1),\n                 tf.expand_dims(tf.repeat(istate[3],repeats=n_chains,axis=-1),axis=-1),\n                 tf.expand_dims(tf.repeat(istate[4],repeats=n_chains,axis=-1),axis=-1)\n                 ]\n\n# Speed up sampling by tracing with `tf.function`.\n@tf.function(autograph=False, jit_compile=True,reduce_retracing=True)\ndef do_sampling():\n  return tfp.mcmc.sample_chain(\n      kernel=adaptive_sampler,\n      current_state=current_state,\n      num_results=num_results,\n      num_burnin_steps=num_burnin_steps,\n      seed= tf.constant([9199, 9999], dtype=tf.int32),\n      trace_fn=None)#lambda current_state, kernel_results: kernel_results)\n\n\nt0 = time.time()\n#samples, kernel_results = do_sampling()\nsamples = do_sampling()\nt1 = time.time()\nprint(\"Inference ran in {:.2f}s.\".format(t1-t0))\n#&gt; Inference ran in 25.51s.\n\nsamples = list(map(lambda x: tf.squeeze(x).numpy(), samples))\nprint(samples)    \n#&gt; [array([[2.987079 , 2.6098182, 2.5677931],\n#&gt;        [2.7483604, 2.886927 , 2.9317207],\n#&gt;        [2.6856432, 2.6786344, 3.1322134],\n#&gt;        ...,\n#&gt;        [3.2533894, 2.801959 , 3.0235026],\n#&gt;        [3.1033385, 3.0212991, 3.3359637],\n#&gt;        [2.9376042, 2.6799467, 3.0788856]], shape=(20000, 3), dtype=float32), array([[1.3236083, 1.3740337, 1.1372954],\n#&gt;        [1.3656137, 1.6774994, 1.1773032],\n#&gt;        [1.5750321, 1.2522798, 1.2792255],\n#&gt;        ...,\n#&gt;        [1.3976835, 1.3620625, 1.4021512],\n#&gt;        [1.4205782, 1.2618387, 1.439176 ],\n#&gt;        [1.4027553, 1.1244894, 1.3615433]], shape=(20000, 3), dtype=float32), array([[-1.0803449 , -0.04994134, -0.6502478 ],\n#&gt;        [-0.78017116, -1.3083422 , -0.68040556],\n#&gt;        [-0.5861852 , -0.8201815 , -0.6553479 ],\n#&gt;        ...,\n#&gt;        [-1.1314462 , -1.2364191 , -1.0025896 ],\n#&gt;        [-1.2382917 , -1.0904466 , -1.3408219 ],\n#&gt;        [-0.76772535, -0.36030054, -1.3073211 ]],\n#&gt;       shape=(20000, 3), dtype=float32), array([[-0.27886984, -0.12817198, -0.05767922],\n#&gt;        [-0.16990772, -0.00638718, -0.1368666 ],\n#&gt;        [-0.22307259, -0.4108208 , -0.6994592 ],\n#&gt;        ...,\n#&gt;        [-0.60789484, -0.06256802, -0.57999694],\n#&gt;        [-0.6803912 , -0.19749126, -0.38116306],\n#&gt;        [-0.55827487, -0.06315253, -0.33170593]],\n#&gt;       shape=(20000, 3), dtype=float32), array([[0.28163257, 0.24762833, 0.25481734],\n#&gt;        [0.24152358, 0.25584832, 0.27553517],\n#&gt;        [0.30509847, 0.26730496, 0.27132562],\n#&gt;        ...,\n#&gt;        [0.2962671 , 0.25178248, 0.29036015],\n#&gt;        [0.24573997, 0.28279573, 0.2581408 ],\n#&gt;        [0.27390787, 0.23220906, 0.29402545]],\n#&gt;       shape=(20000, 3), dtype=float32)]\n\n\n\n\nCode\nsamples&lt;-py$samples\n## Trace plots for the reciprocal dispersion parameter\n#png(\"precomputed/vg2_plot1.png\")\nphi_m&lt;-samples[[5]] # the fifth parameter in the model, a matrix\npar(mfrow=c(2,2))\nplot(phi_m[,1],type=\"l\",col=\"green\",main=\"Trace plots (all chains)\")\nlines(phi_m[,2],col=\"blue\")\nlines(phi_m[,3],col=\"skyblue\")\nplot(phi_m[,1],type=\"l\",col=\"green\",main=\"Trace plots - chain 1\")\nplot(phi_m[,2],type=\"l\",col=\"blue\",main=\"Trace plots - chain 2\")\nplot(phi_m[,3],type=\"l\",col=\"skyblue\",main=\"Trace plots - chain 3\")\n\n\n\n\n\n\n\n\n\nCode\n#dev.off()\n\n\n\n\nCode\n# reloading data as stan does the log(exposure) internally\ndata(roaches)\nroaches$roach1&lt;-roaches$roach1/100;# manual\n\nstan_glm1 &lt;- stan_glm(y ~ roach1 + treatment + senior, offset = log(exposure2),\n                       data = roaches, family = neg_binomial_2,\n                       prior = normal(0, 2.5),\n                       prior_intercept = normal(0, 5),\n                       seed = 12345,\n                       warmup = 10000,      # Number of warmup iterations per chain\n                       iter = 20000,        # Total iterations per chain (warmup + sampling)\n                       thin = 1,\n                       chains = 2)           # Thinning rate)\n#&gt; \n#&gt; SAMPLING FOR MODEL 'count' NOW (CHAIN 1).\n#&gt; Chain 1: \n#&gt; Chain 1: Gradient evaluation took 0.000513 seconds\n#&gt; Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 5.13 seconds.\n#&gt; Chain 1: Adjust your expectations accordingly!\n#&gt; Chain 1: \n#&gt; Chain 1: \n#&gt; Chain 1: Iteration:     1 / 20000 [  0%]  (Warmup)\n#&gt; Chain 1: Iteration:  2000 / 20000 [ 10%]  (Warmup)\n#&gt; Chain 1: Iteration:  4000 / 20000 [ 20%]  (Warmup)\n#&gt; Chain 1: Iteration:  6000 / 20000 [ 30%]  (Warmup)\n#&gt; Chain 1: Iteration:  8000 / 20000 [ 40%]  (Warmup)\n#&gt; Chain 1: Iteration: 10000 / 20000 [ 50%]  (Warmup)\n#&gt; Chain 1: Iteration: 10001 / 20000 [ 50%]  (Sampling)\n#&gt; Chain 1: Iteration: 12000 / 20000 [ 60%]  (Sampling)\n#&gt; Chain 1: Iteration: 14000 / 20000 [ 70%]  (Sampling)\n#&gt; Chain 1: Iteration: 16000 / 20000 [ 80%]  (Sampling)\n#&gt; Chain 1: Iteration: 18000 / 20000 [ 90%]  (Sampling)\n#&gt; Chain 1: Iteration: 20000 / 20000 [100%]  (Sampling)\n#&gt; Chain 1: \n#&gt; Chain 1:  Elapsed Time: 0.731 seconds (Warm-up)\n#&gt; Chain 1:                1.065 seconds (Sampling)\n#&gt; Chain 1:                1.796 seconds (Total)\n#&gt; Chain 1: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'count' NOW (CHAIN 2).\n#&gt; Chain 2: \n#&gt; Chain 2: Gradient evaluation took 1.6e-05 seconds\n#&gt; Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\n#&gt; Chain 2: Adjust your expectations accordingly!\n#&gt; Chain 2: \n#&gt; Chain 2: \n#&gt; Chain 2: Iteration:     1 / 20000 [  0%]  (Warmup)\n#&gt; Chain 2: Iteration:  2000 / 20000 [ 10%]  (Warmup)\n#&gt; Chain 2: Iteration:  4000 / 20000 [ 20%]  (Warmup)\n#&gt; Chain 2: Iteration:  6000 / 20000 [ 30%]  (Warmup)\n#&gt; Chain 2: Iteration:  8000 / 20000 [ 40%]  (Warmup)\n#&gt; Chain 2: Iteration: 10000 / 20000 [ 50%]  (Warmup)\n#&gt; Chain 2: Iteration: 10001 / 20000 [ 50%]  (Sampling)\n#&gt; Chain 2: Iteration: 12000 / 20000 [ 60%]  (Sampling)\n#&gt; Chain 2: Iteration: 14000 / 20000 [ 70%]  (Sampling)\n#&gt; Chain 2: Iteration: 16000 / 20000 [ 80%]  (Sampling)\n#&gt; Chain 2: Iteration: 18000 / 20000 [ 90%]  (Sampling)\n#&gt; Chain 2: Iteration: 20000 / 20000 [100%]  (Sampling)\n#&gt; Chain 2: \n#&gt; Chain 2:  Elapsed Time: 0.732 seconds (Warm-up)\n#&gt; Chain 2:                1.075 seconds (Sampling)\n#&gt; Chain 2:                1.807 seconds (Total)\n#&gt; Chain 2:\nres_m&lt;-as.matrix(stan_glm1)\n\n#png(\"precomputed/vg2_plot2.png\")\npar(mfrow=c(1,1))\nplot(density(c(phi_m)),col=\"skyblue\",lwd=2, main=\"rstanarm (orange) v TF (blue)\",\n        xlab=\"Reciprocal Dispersion\") # all chains combined\nlines(density(res_m[,\"reciprocal_dispersion\"]),col=\"orange\",lwd=2)\n\n\n\n\n\n\n\n\n\nCode\n#dev.off()\n\n\n\n\nCode\nphi_m&lt;-samples[[3]] # the third parameter in the model, a matrix\n#png(\"precomputed/vg2_plot3.png\")\nplot(density(c(phi_m)),col=\"skyblue\",lwd=2, main=\"rstanarm (orange) v TF (blue)\",\nxlab=\"treatment effect\") # all chains combined\nlines(density(res_m[,\"treatment\"]),col=\"orange\",lwd=2)\n\n\n\n\n\n\n\n\n\nCode\n#dev.off()"
  }
]