{
  "hash": "5eb9566c5834ad04f16c1fa925545bc8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian Linear Regression TF v rstanarm\"\noutput:\n  rmarkdown::html_vignette:\n    toc: true\n    toc_depth: 2\n    number_sections: true\nvignette: >\n  %\\VignetteIndexEntry{Bayesian Linear Regression TF v rstanarm}\n  %\\VignetteEngine{knitr::rmarkdown}\n  %\\VignetteEncoding{UTF-8}\n---\n\n\n\n## Key features covered in vignette\n\n-   how to build a simple **Bayesian linear regression model**\n    -   setup the design matrix\n    -   generate **MLE estimates** as initial conditions for MCMC\n-   comparison of numerical results against [rstanarm](https://mc-stan.org/rstanarm/)\n-   show how to match rstanarm's centering of predictors and auto-scaled priors\n\nThis example uses simulated data.\n\n**Click [here](https://github.com/fraseriainlewis/tfclinical/blob/main/assets/fullcodevignettes/intro_normal_tf_stan.orig.Rmd) for the full R Markdown file that generated all the outputs shown here**.\n\n\n\n## Example data set\n\nThis example uses a simple simulated data set generated as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1001)\nnperarm<-25 # N patients per arm\nT1 <-rep(c(0,1),nperarm); # treatment arm 1/0 (binary)\nX1<-rnorm(n=2*nperarm,mean=10,sd=2) # continuous covariate\ny <- rnorm(n=2*nperarm,mean=1+X1+2*T1,sd=2) # Normal response\n\nthedata<-data.frame(y=y,X=X1,T=T1)\n\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_table1_object <- head(thedata) %>%\n  kable() %>%\n  kable_styling(full_width = F) %>%\n  column_spec(1, bold = T, color = \"darkgreen\")\n\n# Save the object\n#saveRDS(my_table1_object, \"precomputed/vg3_table1.rds\")\n\n```\n:::\n\n\n### Example Model - Formulation\n\nStandard additive model with Gaussian response and identity link function with priors for the intercept, slope and scale parameters.\n\n$$\n\\begin{aligned}\n\\text{y}_i &\\sim \\text{N}(\\mu_i, \\phi^2) \\\\\n\\mu_i &= \\alpha + \\beta_{\\text{1}} X_{i} + \\beta_{\\text{2}} T_{i}\\quad\\text{for } i=1,\\dots,N \\\\\n\\alpha &\\sim \\text{N}(0, a_{h}) \\\\\n\\beta_{\\text{1}}, \\beta_{\\text{2}} &\\sim \\text{N}(0, b_{h}) \\\\\n\\sigma &\\sim \\text{Exponential}(c_{h})\n\\end{aligned}\n$$ where $a_h, b_h$ and $c_h$ are fixed hyper-parameters.\n\n## rstanarm\n\nWe first fit the model using [rstanarm](https://mc-stan.org/rstanarm/) library using it's high level interface. When using the rstanarm library it is important to read the specific documentation as:\n\n-   internal predictor centering may be applied by default (including Normal response as here)\n-   predictor centering is also applied to binary variables and factors\n-   some priors maybe auto-scaled by default\n\nCentering does not alter the parameter estimates but does impact how the model is coded, while the second does alter estimates but is designed to do so in a small (and sensible) way. But using TFP in R then it is easy to also utilize the auto-scaled priors computed by RStan (as below), allowing direct numerical comparisons between these two libraries.\n\nWe run the rstanarm Normal regression model and capture the priors used as some of these have been auto-scaled by stan_glm. These same priors will then be used in RStan and TFP. The only parameter auto-scaled is the scale parameter but the code shows how to do this for the slope parameters also (but not needed here).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Note on Rescaling\n# note for models that do internal predictor centering then need location shift back for intercept\n# i.e y = a + b*(x1-mean) + c*(x2-mean)\n# the interpretation of a on non-centered predictors is value of y at x1=0 and x2=0\n# so want a at x1=0 and x2=0 so E(y) = mean(a) + mean(b)*-mean(x1) + mean(c)*-mean(x2) etc.\n\n# Estimate Bayesian version with stan_glm\nstan_glm1 <- stan_glm(y ~ X + T, data = thedata, family = gaussian(),\n                      prior = normal(0, 2.5),\n                      prior_intercept = normal(0, 5),\n                      seed = 12345,\n                      warmup = 10000,      # Number of warmup iterations per chain\n                      iter = 20000,        # Total iterations per chain (warmup + sampling)\n                      thin = 1,\n                      chains = 4,\n                      refresh=0) # turn off iteration output for compactness           \nres_m<-as.matrix(stan_glm1)\n#summary(res_m[,\"(Intercept)\"])\n\n# now get the priors used by rstanarm for use in RStan and TFP\nprior_scales<-prior_summary(stan_glm1)\n# get the predictor adjusted scale - stating priors explicitly so no autoscaling\nbeta_prior_scale<-prior_scales$prior$scale\n#  aux is always re-scaled -next line always needed\nsd_prior_scale<-prior_scales$prior_aux$adjusted_scale #need 1/sd_prior_scale for exp\n\n## now send the observed data and the new priors to python for use later with TFP\npy$thedata<-r_to_py(thedata) \nrescaled_sd=c(beta_prior_scale,1/sd_prior_scale)\npy$rescaled_sd<-r_to_py(rescaled_sd) \n```\n:::\n\n\n## RStan\n\nWe now reproduce the same analysis in RStan rather than rstanarm as this requires more model specific details which can then be implemented in TFP. - we pass the computed priors from rstanarm above which we extracted from the stanfit results object\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# RStan code\nset.seed(12345)\n\n# --- Define the Stan model as a string in R ---\n\nstan_model_string <- \"\ndata {\n  int<lower=1> N;                 // Number of observations\n  int<lower=1> M;                 //number of predictors excl intercept\n  array[N] real<lower=0> y;         // Response variable (counts)\n  vector[N] X;           // Continuous predictor\n  vector[N] T;           // Binary predictor\n  //Hyperparameters\n  array[M] real<lower=0>rescaled_sd;// to hold auto-scaled priors from rstanarm\n\n}\n\ntransformed data {\n  vector[N] X_centered;\n  real mean_X=mean(X);\n  vector[N] T_centered;\n  real mean_T=mean(T);\n  X_centered = X - mean_X;  // Center in transformed data block\n  T_centered = T - mean_T;\n}\n\nparameters {\n  real alpha;                     // Intercept\n  real beta_X;               // Coefficient for roach1\n  real beta_T;            // Coefficient for treatment\n  real<lower=0> phi;              // scale parameter\n}\n\ntransformed parameters {\n  array[N] real mu;           \n  for (i in 1:N) {\n    // this is mean for each data point and will go into likelihood function\n    mu[i] = alpha +\n                beta_X * X_centered[i] +\n                beta_T * T_centered[i] ;\n  }\n}\n\nmodel {\n  // --- Priors ---\n  // Weakly informative priors for coefficients and intercept\n  alpha ~ normal(0, 5.0);           // Prior for intercept\n  // the next parameters are passed from R as taken from rstanarm\n  beta_X ~ normal(0,rescaled_sd[1] );     // Prior for wt coefficient\n  beta_T ~ normal(0, rescaled_sd[2]);  // Prior for am coefficient\n  phi ~ exponential(rescaled_sd[3]); // Prior for scale\n\n  // --- Likelihood ---\n  y ~  normal(mu, phi);\n}\n\ngenerated quantities {\n  // compute intercept paramater back on non-centred axes\n  real intercept_0;\n  intercept_0=alpha + beta_X*-mean_X + beta_T*-mean_T;\n}\n\"\n\n# --- Prepare data for Stan ---\n# The data needs to be provided as a list for rstan::stan()\nstan_data <- list(\n  N = nrow(thedata),\n  M = 3, # number of passed hyperpriors\n  rescaled_sd=c(beta_prior_scale,1/sd_prior_scale),# 1/ as prior uses rate\n  y = thedata$y,\n  X = thedata$X,\n  T = thedata$T\n)\n\n# --- Fit the Stan model ---\n# Use rstan::stan() to compile and sample from the model\nfit <- stan(\n  model_code = stan_model_string,\n  data = stan_data,\n  chains = 4,         # Number of MCMC chains\n  warmup = 10000,      # Number of warmup iterations per chain\n  iter = 20000,        # Total iterations per chain (warmup + sampling)\n  thin = 1,           # Thinning rate\n  seed = 12345,          # For reproducibility\n  refresh=0,\n  control = list(adapt_delta = 0.95, max_treedepth = 15) # Adjust for sampling issues if needed\n)\n#> Trying to compile a simple C file\n#> Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\n#> using C compiler: ‘Apple clang version 17.0.0 (clang-1700.6.3.2)’\n#> using SDK: ‘MacOSX26.2.sdk’\n#> clang -arch arm64 -std=gnu2x -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\n#> In file included from <built-in>:1:\n#> In file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\n#> In file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\n#> In file included from /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n#> /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#>   679 | #include <cmath>\n#>       |          ^~~~~~~\n#> 1 error generated.\n#> make: *** [foo.o] Error 1\n\n# --- 5. Extract main parameters and produce density plots ---\nres2<-extract(fit,par=c(\"alpha\",\"beta_X\",\" beta_T\",\"phi\",\"intercept_0\"))\n\n```\n:::\n\n\n## Comparison of results rstanarm v RStan\n\nThe plots below show a simple comparison of densities between rstanarm and RStan, and it's clear these are virtually identical as we hoped. We now implement the same model into TFP\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro_normal_tf_stan_files/figure-html/plots1-1.png){width=672}\n:::\n:::\n\n\n## TFP\n\nWe now implement the above model in TFP\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport os\nimport keras\n#> /Users/work/tfp/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n#>   if not hasattr(np, \"object\"):\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow_probability import distributions as tfd\ntfb = tfp.bijectors\nimport warnings\nimport time\nimport sys\n\n## The data.frame passed is now a panda df but for TPF this needs to be further\n## converted into tensors here we simply convert each col of the data.frame into\n## a Rank 1 tensor (i.e. a vector)\ny_data=tf.convert_to_tensor(thedata.iloc[:,0], dtype = tf.float32)\nX_data=tf.convert_to_tensor(thedata.iloc[:,1]-np.mean(thedata.iloc[:,1]), dtype = tf.float32)\nT_data=tf.convert_to_tensor(thedata.iloc[:,2]-np.mean(thedata.iloc[:,2]), dtype = tf.float32)\n\nrescaled_sd=tf.convert_to_tensor(rescaled_sd, dtype = tf.float32)\n# rescaled_sd is also available here\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Define LIKELIHOOD - this is defined first, as functions need defined before used\n# The arguments in our function are in the REVERSE ORDER that the parameters\n# appear in JointDistrbutionSequential this is essential\ndef make_observed_dist(phi,beta_T,beta_X,alpha):\n      # Compute linear mean: mu = alpha + beta * x\n      # When sampling 3 times:\n      # alpha: (3,), beta: (3,), x_data: (10,)\n      # Need to broadcast to (3, 10)\n      alpha_expanded = tf.expand_dims(alpha, -1)  # (3, 1)\n      beta_X_expanded = tf.expand_dims(beta_X, -1)    # (3, 1)\n      beta_T_expanded = tf.expand_dims(beta_T, -1)    # (3, 1)\n      mu = alpha_expanded + beta_X_expanded * X_data + beta_T_expanded * T_data  # (3, 1) + (3, 1) * (10,) = (3, 10)\n      # Expand sigma to broadcast\n      phi_expanded = tf.expand_dims(phi, -1)  # (3, 1)\n      # return a vector of Gaussian probabilities, one for each obs\n      return(tfd.Independent(\n        tfd.Normal(loc = mu, scale = phi_expanded), #define as logits\n        reinterpreted_batch_ndims = 1 # This tells TFP that log_prob here is a single value\n                                      # equal to sum of individual log_probs, a vector\n                                      # this is usual when defining a data likelihood\n      ))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# DEFINE FULL MODEL - hyperparams hard coded\n# model is y[i] = Bernoulli(p[i]) where logit(p[i]) = intercept + treatment*z[i]\nmodel = tfd.JointDistributionSequentialAutoBatched([\n  tfd.Normal(loc=0., scale=5.0, name=\"alpha\"),  # prior intercept after centering\n  tfd.Normal(loc=0.,scale=rescaled_sd[[0]], name=\"beta_X\"),   # prior slops\n  tfd.Normal(loc=0., scale=rescaled_sd[[1]], name=\"beta_T\"),  # prior for slope\n  tfd.Exponential(rate=rescaled_sd[[2]], name=\"phi\"), # prior for scale\n  # now finally define the likelihood as we have already defined parameters in this\n  make_observed_dist # the data likelihood defined as a function above\n])\n\ntf.random.set_seed(9999)\n# Sample one variate from this joint probability model - shows what structure the model\n# produces/needs. i.e. what the state variable of the model looks like in TF\nmysample=model.sample(1) # 1 = one random variate\n#print(f\"{mysample}\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(py$mysample)\n#> [[1]]\n#> tf.Tensor([-2.162063], shape=(1), dtype=float32)\n#> \n#> [[2]]\n#> tf.Tensor([-0.47621235], shape=(1), dtype=float32)\n#> \n#> [[3]]\n#> tf.Tensor([0.5767661], shape=(1), dtype=float32)\n#> \n#> [[4]]\n#> tf.Tensor([4.2605343], shape=(1), dtype=float32)\n#> \n#> [[5]]\n#> tf.Tensor(\n#> [[ -4.3874145   -3.3130984   -0.15982056   7.79828     -3.4587793\n#>    -3.1774316   -5.998131    -6.7247553   -4.397325     5.481294\n#>    -9.06718     -5.8871617   -0.2163018   -1.9762998    0.95448446\n#>    -0.760056    -9.579603     3.8263593   -0.6446984  -10.433388\n#>     5.0122633   -4.2138934   -6.930196    -0.45444846  -0.60048294\n#>    -3.3823504    5.5510936   -5.9933496    3.005193     1.27367\n#>    -8.595339    -0.9684988  -10.7711735   -4.492715     4.0445337\n#>     0.06356597  -2.8085861    0.19860196  -1.2367177   -4.812044\n#>     5.1663084    0.9606662   -2.761689    -6.958252    -5.048527\n#>     4.4922323    3.2969918   -4.7693863   -7.007819    -8.508112  ]], shape=(1, 50), dtype=float32)\n```\n:::\n\n\n### Define the log_prog_fn\n\nTo use the various MCMC samplers provided in TFP we need to explicitly provide a function which returns the log of the posterior density. This is straightforward and simply involves wrapping around the existing model method log_prob() as seen below.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# the ordering of the arguments in this must match exactly the ordering used in\n# JointDistributionSequentialAutoBatched\ndef log_prob_fn(alpha,beta_X,beta_T,phi):\n  return model.log_prob((\n      alpha,beta_X,beta_T,phi,y_data))\n                                                         ## last argument is the DATA (y)\n\n# useful to see how to call this. We use the simulated values above\nmylogp=log_prob_fn(mysample[0], mysample[1], mysample[2],mysample[3])\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(py$mylogp)\n#> tf.Tensor([-428.61087], shape=(1), dtype=float32)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef neg_log_prob_fn(pars):\n    alpha=pars[[0]]\n    beta_X=pars[[1]]\n    beta_T=pars[[2]]\n    phi=pars[[3]]\n    \"\"\"Unnormalized target density as a function of states.\"\"\"\n    return -model.log_prob((\n      alpha, beta_X,beta_T,phi, y_data))\n\n\n#### get starting values by find MLE\nif(True):\n    start = tf.constant([0.1,0.1,0.1,0.1],dtype = tf.float32)  # Starting point for the search.\n    optim_results = tfp.optimizer.nelder_mead_minimize(neg_log_prob_fn,\n                 initial_vertex=start, func_tolerance=1e-04,max_iterations=1000)\n\n    print(optim_results.initial_objective_values)\n    print(optim_results.objective_value)\n    print(optim_results.position)\n#> tf.Tensor([380042.6  379745.22 379903.53 380026.53 344706.5 ], shape=(5,), dtype=float32)\n#> tf.Tensor(118.10513, shape=(), dtype=float32)\n#> tf.Tensor([11.955216   0.9763453  2.2308257  2.03263  ], shape=(4,), dtype=float32)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# bijector to map contrained parameters to real\nunconstraining_bijectors = [\n    tfb.Identity(),\n    tfb.Identity(),\n    tfb.Identity(),\n    tfb.Exp()\n]\n\nnum_results=20000\nnum_burnin_steps=10000\n\nsampler = tfp.mcmc.TransformedTransitionKernel(\n    tfp.mcmc.NoUTurnSampler(\n        target_log_prob_fn=log_prob_fn,\n        step_size=tf.cast(0.5, tf.float32)), #tf.cast(0.1, tf.float32)),\n    bijector=unconstraining_bijectors\n    )\n\nadaptive_sampler = tfp.mcmc.DualAveragingStepSizeAdaptation(\n    inner_kernel=sampler,\n    num_adaptation_steps=int(0.8 * num_burnin_steps),\n    target_accept_prob=tf.cast(0.8, tf.float32))\n\nistate = optim_results.position\n\nn_chains=4\ncurrent_state = [tf.expand_dims(tf.repeat(istate[0],repeats=n_chains,axis=-1),axis=-1),\n                 tf.expand_dims(tf.repeat(istate[1],repeats=n_chains,axis=-1),axis=-1),\n                 tf.expand_dims(tf.repeat(istate[2],repeats=n_chains,axis=-1),axis=-1),\n                 tf.expand_dims(tf.repeat(istate[3],repeats=n_chains,axis=-1),axis=-1)\n                 ]\n\n# Speed up sampling by tracing with `tf.function`.\n@tf.function(autograph=False, jit_compile=True,reduce_retracing=True)\ndef do_sampling():\n  return tfp.mcmc.sample_chain(\n      kernel=adaptive_sampler,\n      current_state=current_state,\n      num_results=num_results,\n      num_burnin_steps=num_burnin_steps,\n      seed= tf.constant([9199, 9999], dtype=tf.int32),\n      trace_fn=None)#lambda current_state, kernel_results: kernel_results)\n\n\nt0 = time.time()\n#samples, kernel_results = do_sampling()\nsamples = do_sampling()\nt1 = time.time()\nprint(\"Inference ran in {:.2f}s.\".format(t1-t0))\n#> Inference ran in 3.71s.\n\nsamples = list(map(lambda x: tf.squeeze(x).numpy(), samples))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nalpha = tf.reshape(samples[0], [-1])\nbeta_X = tf.reshape(samples[1], [-1])\nbeta_T = tf.reshape(samples[2], [-1])\nphi = tf.reshape(samples[3], [-1])\n\n# need to re-locate prior back to original - reverse centering\nintercept0=alpha + beta_X*-np.mean(thedata.iloc[:,1]) + beta_T*-np.mean(thedata.iloc[:,2])\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro_normal_tf_stan_files/figure-html/plots2-1.png){width=672}\n:::\n:::\n\n\n",
    "supporting": [
      "intro_normal_tf_stan_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}