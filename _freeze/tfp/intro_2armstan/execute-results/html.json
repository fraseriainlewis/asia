{
  "hash": "30e09d5f32d2121ca4a4cc1a3d152dfd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Two-arm trial TFP v RStan\"\noutput:\n  rmarkdown::html_vignette:\n    toc: true\n    toc_depth: 2\n    number_sections: true\nvignette: >\n  %\\VignetteIndexEntry{Two-arm trial TFP v RStan}\n  %\\VignetteEngine{knitr::rmarkdown}\n  %\\VignetteEncoding{UTF-8}\n---\n\n\n\n## Quick start\n\nThis vignette uses the same model as in [\"Two-arm trial with No U-Turn sampling\"](articles/intro_model_build.html) but now includes a numerical comparison with the same model implemented in RStan. The code chunks here for the TFP model are more compact and slightly re-ordered and abbreviated. The takeaway is that both libraries give similar results, although some tweaking may be required to TFP, e.g. in the target acceptance probability.\n\nOne remark is that RStan readily reports diagnostics such as `divergent transitions`. Similar dignostic output can be captured in TFP via the trace callback (e.g. see the trace fn used below which also captures divergent transitions). RStan has many other diagnostic capabilities such as ESS_bulk and ESS_tail sample sizes which can be readily applied to sampler output from TFP as reticulate makes the samples available from R.\n\n**Click [here](https://github.com/fraseriainlewis/tfclinical/blob/main/assets/fullcodevignettes/intro_2armstan.orig.Rmd) for the full R Markdown file that generated all the outputs shown here**.\n\n### Model Formulation\n\nThe model implemented here is a non-centered parameterization for a logistic model.\n\n$$\n\\begin{aligned}\n\\mu_0 &\\sim \\text{Normal}(0, 2.5)\\\\\n\\sigma_0 &\\sim \\text{Half-Normal}(0, 2.5) \\\\\n\\mu_1 &\\sim \\text{Normal}(0, 2.5) \\\\\n\\sigma_1 &\\sim \\text{Half-Normal}(0, 2.5) \\\\\n\\tilde{\\theta}_j &\\sim \\text{Normal}(0, 1)\\quad\\enspace\\text{for}\\enspace{j=1,2}  \\\\\n\\beta_0 &= \\mu_0 + \\sigma_0 \\tilde{\\theta}_1 \\\\\n\\beta_1 &= \\mu_1 + \\sigma_1 \\tilde{\\theta}_2 \\\\\n\\text{logit(}{p_i}) &= \\beta_0 + \\beta_1 z_i\\quad\\quad\\enspace\\enspace\\text{for}\\enspace{i=1,\\dots,N}  \\\\\ny_i &\\sim \\text{Bernoulli}(p_i)\n\\end{aligned}\n$$\n\n\n\n## Data set\n\nThe R chunk below creates a simple dataset of three cols:\n\n-   a binary response variable (0/1 = non-responder/responder),\n-   a binary treatment variable (0/1 = control/test treatment)\n-   and a basket ID variable. (1)\n\nThe basket ID is currently set fixed at 1, denoting there is only one basket in this trial, i.e. a classical two arm randomized trial design. Baskets will be added in other vignettes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Prepare model inputs\nset.seed(99999)\n# Set up data\nrr_k_ctrl <- c(0.60)        # control response rate for each basket\nrr_k_trt <- c(0.58)         # treatment response rate for each basket\n\nK<-length(rr_k_ctrl)        # number of baskets\n\nN_k_ctrl <- rep(250, K)     # number of control participants per basket\nN_k_trt <- rep(250, K)      # number of treatment participants per basket\nN_k <- N_k_ctrl + N_k_trt   # number of participants per basket (both arms combined)\nN <- sum(N_k)               # total sample size\nk_vec <- rep(1:K, N_k)      # N x 1 vector of basket indicators (1 to K)\n\nz_vec<-NULL;\ny<-NULL;\nfor(i in 1:K){ # for each basket repeat 0-control 1-trt according to the specifc Ns\n  z_vec<-c(z_vec,rep(0:1,c(N_k_ctrl[i],N_k_trt[i]))) # treatment/control indicator\n  y<-c(y,\n       c(rbinom(N_k_ctrl[i],1,rr_k_ctrl[i]), # bernoulli for control\n         rbinom(N_k_trt[i],1,rr_k_trt[i]))) #           for trt\n}\n\nthedata<-data.frame(y,basketID=k_vec,Treatment=z_vec)\n\npy$thedata<-r_to_py(thedata) # THE KEY LINE - makes data available to Python\n                             # this is converted into a pandas dataframe object in Python\n```\n:::\n\n\n\n\n\n## RStan\n\nThe RStan workflow is somewhat more compact than the TFP equivalent, where TFP requires more lower level details to be specified whereas these are dealt with behind the scences in RStan. \\### Model definition This is somewhat simplified for ease of use, e.g. hard coded hyperparameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Stan model\n  BHM_stan_1 <- \"data {\n  int<lower = 0> K;                     // number of baskets (K >= 2)\n  int<lower = 0> N;                     // total number of participants\n  //int<lower = 0, upper = N> N_k[K];     // K x 1 vector of basket sample sizes\n  int<lower = 0, upper = N> N_k;\n  int<lower = 1, upper = K> k_vec[N];   // N x 1 vector of basket indicators\n  int<lower = 0, upper = 1> z_vec[N];   // N x 1 vector of treatment indicators for active treatment arm\n  int<lower = 0, upper = 1> y[N];       // N x 1 vector of binary responses\n}\nparameters {\n  matrix[K,2] beta_tr;                  // K x 2 matrix of transformed regression coefficients\n  real mu1;                              // scalar of hierarchical mean (log odds ratio)\n  real<lower = 0> sigma1;                  // scalar of hierarchical SD (log odds ratio)\n  real mu0;                              // scalar of hierarchical mean (log odds ratio)\n  real<lower = 0> sigma0;                  // scalar of hierarchical SD (log odds ratio)\n\n}\ntransformed parameters {\n  matrix[K,2] beta;                     // K x 2 matrix of regression coefficients (without transformation)\n  // Calculate beta coefs using transformed betas (leads to better mixing)\n  for (k in 1:K){\n    beta[k,1] = mu0 + sigma0 * beta_tr[k,1];\n    beta[k,2] = mu1 + sigma1 * beta_tr[k,2];\n  }\n\n}\nmodel {\n  mu0 ~ normal(0, 2.5);    // vectorized normal priors for hierarchical means (specify SD in normal distn)\n  sigma0 ~ normal(0, 2.5);       // vectorized half-normal priors for hierarchical SDs (specify SD in normal distn)\n  mu1 ~ normal(0, 2.5);    // vectorized normal priors for hierarchical means (specify SD in normal distn)\n  sigma1 ~ normal(0, 2.5);       // vectorized half-normal priors for hierarchical SDs (specify SD in normal distn)\n  beta_tr[,1] ~ normal(0, 1);  // vectorized normal prior for the log odds for the control arm\n  beta_tr[,2] ~ normal(0, 1);  // vectorized normal prior for the log odds ratios\n  for (i in 1:N)\n    y[i] ~ bernoulli_logit(beta[k_vec[i], 1] + beta[k_vec[i], 2] * z_vec[i]);\n}\n\"\n\n```\n:::\n\n\n### Setup and run sampler\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Create list with input values for Stan model\ndata_input_1 <- list(\n  K = K,                # number of subgroups\n  N = N,                # total sample size\n  N_k = N_k,            # sample size per basket\n  k_vec = k_vec,        # N x 1 vector of subgroup indicators\n  z_vec = z_vec,        # N x 1 vector of active treatment arm indicators\n  y = y                # N x 1 vector of binary responses\n  )\n\n\n\n### Compile and fit model\noptions(mc.cores = parallel::detectCores())\n# Compile model (only run the line below once for a simulation study - compilation not dependent on any\n# simulation inputs as defined by scenarios or on simulated data)\nstan_mod_1 <- stan_model(model_code = BHM_stan_1)\n# Fit model\nnsamps <- 10000       # number of posterior samples (after removing burn-in) per chain\nnburnin <- 1000       # number of burn-in samples to remove at beginning of each chain\nnchains <- 4          # number of chains\nBHM_pars_1 <- c(\"mu0\", \"sigma0\",\"mu1\", \"sigma1\", \"beta\",\"beta_tr\")     # parameters to sample\nstart_time <- Sys.time()\nstan_fit_2 <- sampling(stan_mod_1, data = data_input_1, pars = BHM_pars_1,\n                       iter = nsamps + nburnin, warmup = nburnin, chains = nchains)\n#> Warning: There were 109 divergent transitions after warmup. See\n#> https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n#> to find out why this is a problem and how to eliminate them.\n#> Warning: Examine the pairs() plot to diagnose sampling problems\nend_time <- Sys.time()\nend_time - start_time\npost_draws_2 <- as.matrix(stan_fit_2)           # posterior samples of each parameter\n\n```\n:::\n\n\n### Summarize results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesplot)\ncolor_scheme_set(\"viridisA\")\np<-mcmc_trace(stan_fit_2,\"mu0\")\n#ggsave(\"precomputed/vg2_traceplot1_stan.png\", p)\nprint(p)\n```\n\n::: {.cell-output-display}\n![](intro_2armstan_files/figure-html/stanresults-1.png){width=100%}\n:::\n:::\n\n\n\n## Now for the TFP Equivalent Model\n\n### Setup\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport os\nimport keras\n#> /Users/work/tfp/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n#>   if not hasattr(np, \"object\"):\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow_probability import distributions as tfd\ntfb = tfp.bijectors\nimport warnings\nimport time\nimport sys\n\n## The data.frame passed is now a panda df but for TPF this needs to be further\n## converted into tensors here we simply convert each col of the data.frame into\n## a Rank 1 tensor (i.e. a vector)\ny_data=tf.convert_to_tensor(thedata.iloc[:,0], dtype = tf.float32)\nk_vec=tf.convert_to_tensor(thedata.iloc[:,1], dtype = tf.float32)\nz_vec=tf.convert_to_tensor(thedata.iloc[:,2], dtype = tf.float32)\n```\n:::\n\n\n### Model specification\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Define LIKELIHOOD - this is defined first, as functions need defined before used\n# The arguments in our function are in the REVERSE ORDER that the parameters\n# appear in JointDistrbutionSequential this is essential\ndef make_observed_dist(log_odd_control_and_ratio,sigma1, mu1,sigma0,mu0):\n    # beta is a two element tensor, beta[0] parameter for the log odds of the control arm effect\n    #                               beta[1] log odds ratio of treatment to control\n    beta=tf.stack([mu0 + sigma0 * log_odd_control_and_ratio[0],\n                   mu1 + sigma1 * log_odd_control_and_ratio[1]])\n                   #tf.stack is used to stack tensors without creating new tensors\n\n    # return a vector of Bernoulli probabilities, one for each patient, and these estimates\n    # dependent on which arm the patient was randomized to, i.e. two unique values\n    return(tfd.Independent(\n        tfd.Bernoulli(logits=beta[0]+beta[1]*z_vec), #define as logits\n        reinterpreted_batch_ndims = 1 # This tells TFP that log_prob here is a single value\n                                      # equal to sum of individual log_probs, a vector\n                                      # this is usual when defining a data likelihood\n    ))\n    \n    \n# DEFINE FULL MODEL - hyperparams hard coded\n# model is y[i] = Bernoulli(p[i]) where logit(p[i]) = intercept + treatment*z[i]\nmodel = tfd.JointDistributionSequentialAutoBatched([\n  tfd.Normal(loc=0., scale=2.5, name=\"mu0\"),  # prior for mean of control arm log odds\n  tfd.HalfNormal(scale=2.5, name=\"sigma0\"),   # prior for sd of control arm log odds\n  tfd.Normal(loc=0., scale=2.5, name=\"mu1\"),  # prior for mean of treatment log odds ratio\n  tfd.HalfNormal(scale=2.5, name=\"sigma1\"),   # prior for sd of treatment log odds ratio\n  # now come to the standard normals resulting from non-centred parameterization\n  # define these as vector MVN with identity scale matrix\n  tfd.Normal(loc=tf.zeros(2), scale=tf.ones(2), name=\"log_odd_control_and_ratio\"),\n  # now finally define the likelihood as we have already defined parameters in this\n  make_observed_dist # the data likelihood defined as a function above\n])    \n\n# the ordering of the arguments in this must match exactly the ordering used in\n# JointDistributionSequentialAutoBatched\ndef log_prob_fn(mu0, sigma0, mu1,sigma1, log_odd_control_and_ratio):\n  return model.log_prob((\n      mu0, sigma0, mu1,sigma1, log_odd_control_and_ratio,y_data))\n                                                         ## last argument is the DATA (y)\n    \n```\n:::\n\n\n### Setup the MCMC sampling\n\nFirst the initial step sizes and initial conditions\n\n\n::: {.cell}\n\n```{.python .cell-code}\n\n## Adaptive step size.\n## Do in two parts, first a structure to allow step size to adapt separately for\n## each parameter\nsteps=[tf.constant([0.5]),                # mu0\n        tf.constant([0.05]),              # sigma0\n        tf.constant([0.5]),               # mu1\n        tf.constant([0.05]),              # sigma1\n        tf.constant([[0.5,0.5]]) # log_odd_control_and_ratio\n                                 # NOTE - vector and must have correct shape\n        ]\n\n## now we replicate the above \"steps\" array into a structure where this is\n## repeated inside each chain\nn_chains=4 ## THIS SETS NUMBER OF CHAINS\nsteps_chains = [tf.expand_dims(tf.repeat(steps[0],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(steps[1],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(steps[2],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(steps[3],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.tile(steps[4],[n_chains,1]),axis=1) # starts with shape (1,2) i.e. [[a, b]]\n                 ]\n                 \n# initial conditions\nistate=[tf.constant([0.0]),\n        tf.constant([0.5]),\n                   tf.constant([0.0]),\n        tf.constant([0.5]),\n        tf.constant([[1.,-1.]])]\n\ncurrent_state = [tf.expand_dims(tf.repeat(istate[0],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(istate[1],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(istate[2],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.repeat(istate[3],repeats=n_chains,axis=-1),axis=-1), # starts with shape (1,)\n                 tf.expand_dims(tf.tile(istate[4],[n_chains,1]),axis=1) # starts with shape (1,2) i.e. [[a, b]]\n                 ]\n                 \n```\n:::\n\n\nNow the sampler structure, bijectors, step size adaptation and the technical parameters for No U-Turn.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# bijectors which define the mapping from **unconstrained space to target space**\n# i.e. Exp() is used for scale as this maps R -> R+\nunconstraining_bijectors = [\n    tfb.Identity(),  # mu0\n    tfb.Exp(),       # sigma0\n    tfb.Identity(),  # mu1\n    tfb.Exp(),       # sigma0\n    tfb.Identity()  # log_odd_control_and_ratio - this is a vector parameter\n]\n\nnum_results=10000 # number of steps to run each chain for AFTER burn-in\nnum_burnin_steps=1000 # this is discarded (currently not other option in TPF)\n\nmysampler=tfp.mcmc.NoUTurnSampler(\n                                     target_log_prob_fn=log_prob_fn,\n                                     max_tree_depth=15, # default is 10\n                                     max_energy_diff=1000.0, # default do not change\n                                     step_size=steps_chains\n                                     )\n\nsampler = tfp.mcmc.TransformedTransitionKernel( # inside this the starting conditions must be on\n                                                # original scale i.e. precisions must be >0\n    mysampler,\n    bijector=unconstraining_bijectors\n    )\n\n## define final sampler - NUTS, bijectors and adaptation\nadaptive_sampler = tfp.mcmc.DualAveragingStepSizeAdaptation(\n    inner_kernel=sampler,\n    num_adaptation_steps=int(0.8 * num_burnin_steps),\n    reduce_fn=tfp.math.reduce_logmeanexp, # default - this determines how to change the step\n                                          # adaptation across chains\n    #reduce_fn=tfp.math.reduce_log_harmonic_mean_exp, # might be better if difficult chains\n    target_accept_prob=tf.cast(0.95, tf.float32)) # this is a key parameter to get good mixing\n```\n:::\n\n\nNow define the trace/monitoring function.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef trace_fn(state, pkr):\n    return {\n        'sample': state, # this is also pkr['all'][0].transformed_state[0]\n        'step_size': pkr.new_step_size,  # <--- THIS extracts the adapted step size\n        'all': pkr, #state is also pkr['all'][0].transformed_state[0]\n        #pkr is a named tuple with ._fields = ('transformed_state', 'inner_results')\n        # 'transformed_state is the state\n        # 'inner_results' is diagnostics\n        # ('target_log_prob', 'grads_target_log_prob', 'step_size', 'log_accept_ratio', 'leapfrogs_taken',                     'is_accepted', 'reach_max_depth', 'has_divergence', 'energy', 'seed')\n        'has_divergence':pkr[0].inner_results.has_divergence,\n        'logL': log_prob_fn(state[0], state[1],state[2],state[3],state[4])\n    }\n```\n:::\n\n\n### Run the actual sampler\n\nThe number of steps and burn-in were defined above when we setup the No U-Turn sampler.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Speed up sampling by tracing with `tf.function`.\n@tf.function(autograph=False, jit_compile=True,reduce_retracing=True)\ndef do_sampling():\n  return tfp.mcmc.sample_chain(\n      kernel=adaptive_sampler,\n      current_state=current_state,\n      num_results=num_results,\n      num_burnin_steps=num_burnin_steps,\n      seed= tf.constant([9999, 9999], dtype=tf.int32), # this is random seed\n      trace_fn=trace_fn)\n\n\nt0 = time.time()\n#samples, kernel_results = do_sampling()\nsamples, traceout = do_sampling()\n#res = do_sampling()\n#[mu0, sigma0, mu1, sigma1,log_odds_control_and_ratio], results = do_sampling()\nt1 = time.time()\nprint(\"Inference ran in {:.2f}s.\".format(t1-t0))\n#> Inference ran in 166.91s.\n\n## there is a trailing dimension of 1 so need to squeeze to get each col is chain, and each row is parameter sample\nsamplesflat = list(map(lambda x: tf.squeeze(x).numpy(), samples))\n```\n:::\n\n\n### Some Output plots\n\nWe plot the log-likelihood over the MCMC steps (post-burn-in) using two chains, a different colour for each chain. Similarly for trace plots. These are just illustrative output examples, the number of burn-in and total chain steps here are too low for reliable estimation.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nplt.figure(figsize=(10, 5))\nplt.plot(samplesflat[0]) # mu_b\n#plt.savefig(f\"precomputed/vg2_traces.png\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](intro_2armstan_files/figure-html/outputs2-1.png){width=960}\n:::\n:::\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](intro_2armstan_files/figure-html/compare-3.png){width=672}\n:::\n:::\n\n\n",
    "supporting": [
      "intro_2armstan_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}